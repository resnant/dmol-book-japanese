
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Deep Learning for Molecules &amp; Materials Book" lang="en" name="description" xml:lang="en" />
<meta content="en_US" property="og:locale" />
<meta content="summary" name="twitter:card" />
<meta content="Deep Learning for Molecules &amp; Materials Book" name="twitter:description" />
<meta content="dmol.pub 📖" name="twitter:title" />
<meta content="https://dmol.pub/_static/logo.png" name="twitter:image" />
<meta content="&#64;andrewwhite01" name="twitter:site" />

    <title>8. Graph Neural Networks &#8212; deep learning for molecules &amp; materials</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://resnant.github.io/dmol-book-japanese/dl/gnn.html" />
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Input Data &amp; Equivariances" href="data.html" />
    <link rel="prev" title="7. Standard Layers" href="layers.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">deep learning for molecules & materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   この本の概要
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   2. 機械学習入門
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   3. Regression &amp; Model Assessment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   4. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/kernel.html">
   5. カーネル学習
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   6. ディープラーニングの概要
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   7. Standard Layers
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   9. Input Data &amp; Equivariances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Equivariant.html">
   10. Equivariant Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="molnets.html">
   11. Modern Molecular NNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="xai.html">
   12. 予測を説明する
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   13. アテンションレイヤー
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   15. Deep Learning on Sequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VAE.html">
   16. Variational Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="flows.html">
   17. Normalizing Flows
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  D. Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/QM9.html">
   18. GNNによるDFTエネルギーの予測
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../applied/MolGenerator.html">
   19. Generative RNN in Browser
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  E. Contributed Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Hyperparameter_tuning.html">
   20. Hyperparameter Tuning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  F. Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../style.html">
   21. Style Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   22. Changelog
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <script async defer src="https://api.dmol.pub/latest.js"></script><noscript><img src="https://api.dmol.pub/noscript.gif" alt="" referrerpolicy="no-referrer-when-downgrade" /></noscript> By <a href="https://twitter.com/andrewwhite01">Andrew White</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/dl/gnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/resnant/dmol-book-japanese/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/resnant/dmol-book-japanese//issues/new?title=Issue%20on%20page%20%2Fdl/gnn.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/resnant/dmol-book-japanese/blob/master/dl/gnn.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   8.1. グラフの表現
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   8.2. このノートブックの動かし方
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   8.3. グラフニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnn">
     8.3.1. シンプルなGNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kipf-welling-gcn">
   8.4. Kipf &amp; Welling GCN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcn">
     8.4.1. GCNの実装
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   8.5. 例：溶解度予測
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passinggcn">
   8.6. Message PassingとGCN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-graph-neural-network">
   8.7. Gated Graph Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pooling">
   8.8. Pooling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readout-function">
   8.9. Readout Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intensive-vs-extensive">
     8.9.1. Intensive vs Extensive
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#battaglia-general-equations">
   8.10. Battaglia General Equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#battaglia-equationsgcn">
     8.10.1. Battaglia equationsによるGCNの再定式化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-schnet-architecture">
   8.11. The SchNet Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#schnet-example-predicting-space-groups">
   8.12. SchNet Example: Predicting Space Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-the-graphs">
     8.12.1. Building the graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-mlps">
     8.12.2. Implementing the MLPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-gnn">
     8.12.3. Implementing the GNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     8.12.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-research-directions">
   8.13. Current Research Directions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-architecture-motifs-and-comparisons">
     8.13.1. Common Architecture Motifs and Comparisons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nodes-edges-and-features">
     8.13.2. Nodes, Edges, and Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-message-passing">
     8.13.3. Beyond Message Passing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-we-need-graphs">
     8.13.4. Do we need graphs?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stereochemistry-chiral-molecules">
     8.13.5. Stereochemistry/Chiral Molecules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   8.14. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intro-to-gnns">
     8.14.1. Intro to GNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview-of-gnn-with-molecule-compiler-examples">
     8.14.2. Overview of GNN with Molecule, Compiler Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   8.15. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   8.16. Cited References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Graph Neural Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   8.1. グラフの表現
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   8.2. このノートブックの動かし方
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   8.3. グラフニューラルネットワーク
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gnn">
     8.3.1. シンプルなGNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kipf-welling-gcn">
   8.4. Kipf &amp; Welling GCN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcn">
     8.4.1. GCNの実装
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   8.5. 例：溶解度予測
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passinggcn">
   8.6. Message PassingとGCN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-graph-neural-network">
   8.7. Gated Graph Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pooling">
   8.8. Pooling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readout-function">
   8.9. Readout Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intensive-vs-extensive">
     8.9.1. Intensive vs Extensive
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#battaglia-general-equations">
   8.10. Battaglia General Equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#battaglia-equationsgcn">
     8.10.1. Battaglia equationsによるGCNの再定式化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-schnet-architecture">
   8.11. The SchNet Architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#schnet-example-predicting-space-groups">
   8.12. SchNet Example: Predicting Space Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-the-graphs">
     8.12.1. Building the graphs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-mlps">
     8.12.2. Implementing the MLPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-the-gnn">
     8.12.3. Implementing the GNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     8.12.4. Training
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-research-directions">
   8.13. Current Research Directions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-architecture-motifs-and-comparisons">
     8.13.1. Common Architecture Motifs and Comparisons
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nodes-edges-and-features">
     8.13.2. Nodes, Edges, and Features
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beyond-message-passing">
     8.13.3. Beyond Message Passing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#do-we-need-graphs">
     8.13.4. Do we need graphs?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stereochemistry-chiral-molecules">
     8.13.5. Stereochemistry/Chiral Molecules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relevant-videos">
   8.14. Relevant Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intro-to-gnns">
     8.14.1. Intro to GNNs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview-of-gnn-with-molecule-compiler-examples">
     8.14.2. Overview of GNN with Molecule, Compiler Examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   8.15. Chapter Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   8.16. Cited References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="graph-neural-networks">
<h1><span class="section-number">8. </span>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>歴史的に、機械学習で分子を扱う際の最大の課題は、記述子の選定とその計算でした。グラフニューラルネットワーク（GNN）は、グラフを入力とするディープニューラルネットワークの一種であり、GNNは分子を直接入力として受け取ることができるため、記述子に頭を悩ませる必要がありません。</p>
<!-- Historically, the biggest difficulty for machine learning with molecules was the choice and computation of "descriptors". Graph neural networks (GNNs) are a category of deep neural networks whose inputs are graphs and provide a way around the choice of descriptors. A GNN can take a molecule directly as input. -->
<div class="admonition- admonition">
<p class="admonition-title">本章の想定読者と目的</p>
<p>この章では、<a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> と <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a> の内容は前提としています。本章ではグラフやGNNの定義から説明しますが、グラフやニューラルネットワークの基本的な概念について予め慣れているとなお良いでしょう。この章を学ぶことで、以下ができるようになります：</p>
<ul class="simple">
<li><p>分子をグラフで表現できる</p></li>
<li><p>一般的なGNNのアーキテクチャを議論したり、GNNの種類を理解できる</p></li>
<li><p>GNNを構築し、ラベルの種類に応じた読み出し関数（read-out function）を選択できる</p></li>
<li><p>グラフ、エッジ、ノードの特徴をそれぞれ区別できる</p></li>
<li><p>GNNをエッジ更新、ノード更新、集約の各ステップに分けて定式化できる</p></li>
</ul>
</div>
<p>GNNはグラフを入力および出力するために特別に設計されたレイヤーです。GNNについてのレビューは複数執筆されており、例えば Dwivedi <em>et al.</em><span id="id1">[<a class="reference internal" href="#id98" title="Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.">DJL+20</a>]</span>, Bronstein <em>et al.</em><span id="id2">[<a class="reference internal" href="#id99" title="Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.">BBL+17</a>]</span>, Wu <em>et al.</em><span id="id3">[<a class="reference internal" href="#id100" title="Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020.">WPC+20</a>]</span> などが挙げられます。
GNNは、粗視化分子動力学シミュレーション <span id="id4">[<a class="reference internal" href="#id112" title="Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. Chemical Science, 11(35):9524–9531, 2020.">LWC+20</a>]</span> からNMRの化学シフト予測 <span id="id5">[<a class="reference internal" href="#id113" title="Ziyue Yang, Maghesree Chakraborty, and Andrew D White. Predicting chemical shifts with graph neural networks. bioRxiv, 2020.">YCW20</a>]</span> 、固体のダイナミクスのモデリング <span id="id6">[<a class="reference internal" href="#id59" title="Tian Xie, Arthur France-Lanord, Yanming Wang, Yang Shao-Horn, and Jeffrey C Grossman. Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials. Nature communications, 10(1):1–9, 2019.">XFLW+19</a>]</span> まで、あらゆるアプリケーションに適用できます。
GNNについて深く踏み込む前に、まずグラフがコンピュータ上でどのように表現され、分子がどのようにグラフに変換されるか理解しましょう。</p>
<p>グラフとGNNについてのインタラクティブな入門資料が、 <a class="reference external" href="https://distill.pub/2021/gnn-intro/">distill.pub</a> <span id="id7">[<a class="reference internal" href="#id195" title="Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alex Wiltschko. A gentle introduction to graph neural networks. Distill, 2021. https://distill.pub/2021/gnn-intro. doi:10.23915/distill.00033.">SLRPW21</a>]</span> で提供されています。現在のGNNの研究のほとんどは、グラフに特化したディープラーニングライブラリを用いて行われており、2022年現在最も代表的なライブラリは <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a>, <a class="reference external" href="https://www.dgl.ai/">Deep Graph library</a>, <a class="reference external" href="https://graphneural.network/">Spektral</a>, <a class="reference external" href="https://github.com/tensorflow/gnn">TensorFlow GNNS</a> などです。</p>
<section id="id8">
<h2><span class="section-number">8.1. </span>グラフの表現<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>グラフ <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> は、ノード <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> およびエッジ <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>　の集合です。 我々のセッティングでは、ノード <span class="math notranslate nohighlight">\(i\)</span> はベクトル <span class="math notranslate nohighlight">\(\vec{v}_i\)</span> で定義されるので、ノードの集合はランク2のテンソルとして表現できます。
エッジは隣接行列（adjacency matrix） <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> で表現され、もし <span class="math notranslate nohighlight">\(e_{ij} = 1\)</span> であればノード <span class="math notranslate nohighlight">\(i\)</span> と <span class="math notranslate nohighlight">\(j\)</span> がエッジで結合しているとみなされます。
グラフを扱う多くの分野において、簡単のため、グラフはしばしば有向非巡回グラフ（エッジには向きがあるが、一周して元のノードには戻らない）であると仮定されます。しかし、分子において結合には向きが無く、輪を持つ（巡回する）場合もあることに注意してください。化学結合において向きの概念はないことから、我々が扱う隣接行列は常に対称（<span class="math notranslate nohighlight">\(e_{ij} = e_{ji}\)</span>）となります。また、しばしばエッジ自身も特徴を持つ場合があり、 <span class="math notranslate nohighlight">\(e_{ij}\)</span> 自体をベクトルとすることで表現します。この場合は隣接行列はランク3のテンソルとなります。エッジ特徴の例としては、共有結合の次数や、2つのノード間の距離（＝原子間距離）などが挙げられます。</p>
<figure class="align-default" id="methanol">
<a class="reference internal image-reference" href="../_images/methanol.jpg"><img alt="../_images/methanol.jpg" src="../_images/methanol.jpg" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">グラフに変換できるよう、メタノールの各原子に番号を割り当てた</span><a class="headerlink" href="#methanol" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title">one-hot</p>
<p>one-hotベクトルとは、1つの要素が1で、それ以外の要素は全て0であるベクトルのことと思い出してください。例えば <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0]</span></code>
1の要素がクラスを表します。今回の場合、クラスとは元素のことであり、各原子の元素をone-hotベクトルで表しています。</p>
</aside>
<p>では、分子からどのようにグラフを構築できるか見てみましょう。例として、メタノールを考えます（ <a class="reference internal" href="#methanol"><span class="std std-numref">Fig. 8.1</span></a> ）。ノードとエッジを定義するため、便宜的に各原子に番号を振りました。まずはじめはノード特徴を考えます。ノードの特徴量には何を使っても良いのですが、多くの場合、one-hotエンコーディングされた特徴ベクトルを使うことになるでしょう：</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Node</p></th>
<th class="head"><p>C</p></th>
<th class="head"><p>H</p></th>
<th class="text-align:right head"><p>O</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span> が、これらのノードについての結合された特徴ベクトルになります。このグラフの近接行列 <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> は次のようになるでしょう：</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>多少時間をかけてもよいので、これら2つをしっかりと理解してください。例えば、1,2,3行目については、4列目の成分だけが0でないことに注目してください。これは、原子1〜3は炭素（原子4）にのみ結合しているからです。また、原子は自分自身とは結合できないので、対角成分は常に0になります。</p>
<p>分子だけでなく、結晶構造についても似た方法でグラフ化できます。これについては Xie et al.による <span id="id9">[<a class="reference internal" href="#id136" title="Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Phys. Rev. Lett., 120:145301, Apr 2018. URL: https://link.aps.org/doi/10.1103/PhysRevLett.120.145301, doi:10.1103/PhysRevLett.120.145301.">XG18</a>]</span> を参照してください。</p>
<p>それでは、SMILESによる分子の文字列表現をグラフに変換する関数を定義するところから始めましょう。</p>
</section>
<section id="id10">
<h2><span class="section-number">8.2. </span>このノートブックの動かし方<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>このページ上部の  <i aria-label="Launch interactive content" class="fas fa-rocket"></i>  を押すと、このノートブックがGoogle Colab.で開かれます。必要なパッケージのインストール方法については以下を参照してください。</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>必要なパッケージをインストールするには、新規セルを作成して次のコードを実行してください。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install dmol-book
</pre></div>
</div>
<p>もしインストールがうまくいかない場合、パッケージのバージョン不一致が原因である可能性があります。動作確認がとれた最新バージョンの一覧は<a class="reference external" href="https://github.com/whitead/dmol-book/blob/master/package/requirements.txt">ここ</a>から参照できます</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">rdkit</span><span class="o">,</span> <span class="nn">rdkit.Chem</span><span class="o">,</span> <span class="nn">rdkit.Chem.rdDepictor</span><span class="o">,</span> <span class="nn">rdkit.Chem.Draw</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">dmol</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv&quot;</span>
<span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">my_elements</span> <span class="o">=</span> <span class="p">{</span><span class="mi">6</span><span class="p">:</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;H&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>下の非表示セルでは、関数 <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> を定義しています。この関数は元素C, H, Oについてone-hotなノード特徴ベクトルを生成します。また同時に、このone-hotベクトルを特徴ベクトルとする隣接テンソルを生成します。</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">my_elements</span><span class="p">)))</span>
    <span class="n">lookup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">my_elements</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">lookup</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">())]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;Ignoring bond order&quot;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">nodes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h2><span class="section-number">8.3. </span>グラフニューラルネットワーク<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>グラフニューラルネットワーク (GNN) は、次の2つの特徴を持つニューラルネットワークです。</p>
<ol class="simple">
<li><p>入力がグラフである</p></li>
<li><p>出力は順序等価（permutation equivariant）である</p></li>
</ol>
<p>最初の点は明らかですが、2つめの特徴は説明が必要でしょう。まず、グラフの並べ替えとは、ノードを並べ替えることを意味します。
上のメタノールの例では、炭素を原子4ではなく原子1にすることも簡単にできます。その場合、新しい隣接行列は次のようになります：</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>この隣接行列の交換に正しく対応してGNNの出力が変換するなら、そのGNNは順序等価であると言えます。部分電荷や化学シフトのように原子ごとに定義される量をモデリングする場合、このような順序不変の仕組みは不可欠です。つまり、もし原子を入力する順序を変えれば、予測される部分電荷の順序も同様に変わってほしいのです。</p>
<p>もちろん、溶解度やエネルギーのように分子全体の特性をモデリングしたい場合もあります。これらの量は原子の順番を変えても <strong>不変（invariant）</strong> であるべきです。順序について等価（equivariant）なモデルを不変（invariant）にするため、後に定義するリードアウト（read-out）を使います。等価性についてのより詳細な議論は <a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a> を参照してください。</p>
<section id="gnn">
<h3><span class="section-number">8.3.1. </span>シンプルなGNN<a class="headerlink" href="#gnn" title="Permalink to this headline">¶</a></h3>
<p>我々はこれからGNNに言及しますが、実際にはGNN全体ではなく特定のレイヤーのことを指します。多くのGNNはグラフを取り扱うために特別に設計されたレイヤーを備えており、通常はこのレイヤーについてのみ関心を持ちます。それでは、GNNの簡単なレイヤーの例を見てみましょう:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d6dfebc7-5478-42c1-bed9-1d0ac1ee48a0">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-d6dfebc7-5478-42c1-bed9-1d0ac1ee48a0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i \sum_j v_{ij}w_{jk}  \right)
\end{equation}\]</div>
<p>この式は、まず各ノード（<span class="math notranslate nohighlight">\(v_{jij}\)</span>）の特徴に学習可能な重み <span class="math notranslate nohighlight">\(w_{jk}\)</span> をかけた後、全てのノードの特徴を合計し、活性化を適用することを表しています。この操作により、グラフに対して1つの特徴ベクトルが得られます。では、この式は順序等価でしょうか？答えはYesです。なぜならこの式においてノードインデックスはインデックス <span class="math notranslate nohighlight">\(i\)</span> であり、出力に影響を与えることなく順序の並べ替えが可能であるためです。</p>
<p>では次に、この例と似ているが順序等価ではない例を見てみましょう。</p>
<div class="amsmath math notranslate nohighlight" id="equation-7efd834e-1cc7-4431-8380-6b0814a497db">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-7efd834e-1cc7-4431-8380-6b0814a497db" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i v_{ij}w_{ik}  \right)
\end{equation}\]</div>
<p>これは小さな変化です。いま、ノードごとに1つの重みベクトルがあります。したがって、学習可能な重みはノードの順序に依存します。次に、ノードの順序を入れ替えると、学習した重みはノードに対応しなくなります。よって、ノード（＝原子）の順番を変えた2つのメタノール分子を入力すると、異なる出力が得られます。実際のところ、この単純な例は2つの点で実際のGNNと異なります。1つめは単一の特徴ベクトルを出力してノードごとの情報を捨ててしまっている点、2つめは隣接行列を使用しない点です。では、順序等価を維持し、かつこれら2つの性質を備えた実際のGNNを見てみましょう。</p>
</section>
</section>
<section id="kipf-welling-gcn">
<h2><span class="section-number">8.4. </span>Kipf &amp; Welling GCN<a class="headerlink" href="#kipf-welling-gcn" title="Permalink to this headline">¶</a></h2>
<p>初期に人気があったGNNの一つは、Kipf &amp; Welling graph convolutional network (GCN) <span id="id12">[<a class="reference internal" href="#id77" title="Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.">KW16</a>]</span> です。GCNをGNNの広いクラスの一つとして考える人もいますが、本書でGCNとは特にKipf &amp; Welling GCNを指すものとします。
Thomas Kipfは、<a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">GCNの優れた紹介記事</a>を書いています。</p>
<p>GCNレイヤーへの入力はノードおよびエッジの集合（訳注：各ノードおよびエッジはベクトルで表現されるので、これらの集合はテンソルです） <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> で、出力は更新されたノードの集合 <span class="math notranslate nohighlight">\(\mathbf{V}'\)</span> です。 各ノード特徴の更新は、 <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> により表現される近傍ノードの特徴ベクトルを平均することでなされます。</p>
<!-- todo: Eについての記述を誤読していないか見直す。原文は The way it updates a node feature vector is by averaging the feature vectors of its neighbors, as determined by $\mathbf{E}$. -->
<p>近傍ノードの情報を平均することで、GCNレイヤーはノードについて順序等価になっています。近傍について平均するという操作そのものは学習できないので、特徴ベクトルを加算してからノードの次数（結合している近傍ノード数）で除算することで計算します。平均をとる前に、学習可能な重み行列を近傍特徴に掛けることにします。これによりGCNがデータから学習することが可能になります（訳注：言い換えれば、GCNの学習とはこの重み行列の要素を最適化することです）。この操作は次のように記述されます：</p>
<div class="math notranslate nohighlight" id="equation-gcn">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-gcn" title="Permalink to this equation">¶</a></span>\[
v_{il} = \sigma\left(\frac{1}{d_i}e_{ij}v_{jk}w_{lk}\right)
\]</div>
<p><span class="math notranslate nohighlight">\(i\)</span> は着目しているノード、 <span class="math notranslate nohighlight">\(j\)</span> はその近傍インデックス, <span class="math notranslate nohighlight">\(k\)</span> はノードの入力特徴、 <span class="math notranslate nohighlight">\(l\)</span> はノードの出力特徴、 <span class="math notranslate nohighlight">\(d_i\)</span> はノードの次数（次数で割ることで単なる加算ではなく平均になります）、 <span class="math notranslate nohighlight">\(e_{ij}\)</span> は、全ての非近傍ノードが <span class="math notranslate nohighlight">\(v_{jk}\)</span> ゼロになるよう近傍と非近傍ノードを分離する項、 <span class="math notranslate nohighlight">\(\sigma\)</span> は活性化関数、 <span class="math notranslate nohighlight">\(w_{lk}\)</span> は学習可能な重みです。
この式はとても長いように見えますが、実際にやっていることは、近傍同士の平均に学習可能な重み行列を追加しただけです。この式のよくある拡張として、各ノードの近傍として自分自身も加える場合があります。これはノードの出力特徴 <span class="math notranslate nohighlight">\(v_{il}\)</span> が入力特徴 <span class="math notranslate nohighlight">\(v_{ik}\)</span> に依存するようにするためです。しかし、我々はこのために上の式を修正する必要はありません。もっとシンプルなやり方として、データの前処理で恒等行列を加算し、近接行列の対角成分を <span class="math notranslate nohighlight">\(0\)</span> ではなく<span class="math notranslate nohighlight">\(1\)</span> にしてやればよいのです。</p>
<p>GCNについての理解を深めることは、他の種類のGNNを理解するために重要です。ここでは2つのポイントを押さえてください。まずGCNレイヤーは、ノードとその近傍の間で”通信”する方法と見ることができます。ノード <span class="math notranslate nohighlight">\(i\)</span> についての出力は、そのすぐ隣のノードにのみ依存することになりますが、化学の場合、これでは不十分です。より遠方のノードの情報を取り込むために、我々は複数のGCNレイヤーを重ねることができます。もし2つのGCNレイヤーがあれば、ノード <span class="math notranslate nohighlight">\(i\)</span> の出力は、その隣の隣のノードの情報も含むことになります。
GCNで理解すべきもう一つの点は、ノード特徴を平均化するステップが2つの目的を達していることです：i)近傍ノードの順序を無視することで順序に対して等価となる、ii)ノード特徴のデータのスケール変化を防ぐ。単にノード特徴の和を取った場合、(i)は実現しますが、各レイヤーを通すごとにノード特徴のデータの値が大きくなってしまいます。もちろん、特徴のスケールを揃えるために各GCNレイヤーの後でバッチ平均化（batch normalization）レイヤーを通すという対処もありますが、平均化はよりシンプルです。</p>
<figure class="align-default" id="dframe">
<div class="cell_output docutils container">
<img alt="../_images/gnn_11_0.png" src="../_images/gnn_11_0.png" />
</div>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">グラフ畳み込みレイヤーの中間ステップ。3次元ベクトルはノード特徴であり、初期値は水素を表すone-hotベクトル <code class="docutils literal notranslate"><span class="pre">[1.00,</span> <span class="pre">0.00,</span> <span class="pre">0.00]</span></code> です。この中心ノードは、隣接するノードの特徴を平均することで更新されていきます。</span><a class="headerlink" href="#dframe" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>GCNレイヤーを理解しやすくするため、 <a class="reference internal" href="#dframe"><span class="std std-numref">Fig. 8.2</span></a> を見てください。これはGCNレイヤーの中間ステップを表しています。各ノードの特徴は、ここではone-hot encodingされたベクトルとして表現されています。<a class="reference internal" href="#gcnanim"><span class="std std-numref">Fig. 8.3</span></a> のアニメーションは、近傍特徴についての平均化プロセスを表しています。このアニメーションでは、わかりやすくするために学習可能な重みと活性化関数は記述されていません。このアニメーションは2層目のGCNレイヤーでも繰り返されることに注意してください。分子中に酸素原子が含まれるという”情報”が、2層目ではじめて各原子に伝搬される様子をよく見てください。全てのGNNは似たようなアプローチで動作するので、このアニメーションの内容はとても大切です。ぜひ、よく理解してください。</p>
<figure class="align-default" id="gcnanim">
<img alt="../_images/gcn.gif" src="../_images/gcn.gif" />
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">グラフ畳み込み層の動作のアニメーション。左が入力、右が出力ノードの特徴です。2つの層が表示されていることに注意してください（タイトルが変わることに注意して見てください）。アニメーションが進むにつれて、近傍ノードの平均化によって、原子についての情報がどのように分子内を伝播していくかがわかることでしょう。つまり、酸素は単なる酸素から、CとHに結合した酸素、HとCH3に結合した酸素へと変化していくのです。図中の色は数値に対応しています。</span><a class="headerlink" href="#gcnanim" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="gcn">
<h3><span class="section-number">8.4.1. </span>GCNの実装<a class="headerlink" href="#gcn" title="Permalink to this headline">¶</a></h3>
<p>それでは、GCNのテンソル実装を作りましょう。ここでは一旦、活性化関数および学習可能な重みについては省略します。
まず最初に、ランク2の隣接行列を計算する必要があります。上の <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> コードは、特徴ベクトルを用いて隣接行列を計算します。この計算は簡単です。同時に恒等行列を加えることにします（訳注：この恒等行列を加算する操作は、上で述べたように自分自身を隣接ノードとして取り扱うための工夫です）。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">adj_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">adj_mat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1., 1., 1., 1., 0.],
       [1., 1., 0., 0., 0., 1.],
       [1., 0., 1., 0., 0., 0.],
       [1., 0., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
<p>各ノードの次数を計算するために、また別な縮約操作を行います：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj_mat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">degree</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5., 3., 2., 2., 2., 2.])
</pre></div>
</div>
</div>
</div>
<p>これでノードの更新操作の準備ができました。アインシュタインの縮約記法を使って更新操作を表現すると次のようになります</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># note to divide by degree, make the input 1 / degree</span>
<span class="n">new_nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,ij,jk-&gt;ik&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1. 0. 0.]
[0.2 0.2 0.6]
</pre></div>
</div>
</div>
</div>
<p>これをKerasのLayerとして実装するには、上記のコードを新しいLayerのサブクラスとして記述する必要があります。今回のコードは比較的簡単ですが、Kerasの関数名とLayerクラスについて、<a class="reference external" href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/">このチュートリアル</a>を読んで学ぶことを推奨します。主な変更点は、学習可能なパラメータ <code class="docutils literal notranslate"><span class="pre">self.w</span></code> を作って <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/einsum" title="(in TensorFlow v2.8)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.einsum</span></code></a> の中で用いること、活性化関数 <code class="docutils literal notranslate"><span class="pre">self.activation</span></code> を用いること、そして新しいノード特徴と隣接行列を出力することの3点です。隣接行列を出力する理由は、隣接行列を毎回渡すことなく、複数のGCNレイヤーをスタックできるようにするためです。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GCNLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of GCN as layer&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># constructor, which just calls super constructor</span>
        <span class="c1"># and turns requested activation into a callable function</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCNLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="c1"># create trainable weights</span>
        <span class="n">node_shape</span><span class="p">,</span> <span class="n">adj_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># split input into nodes, adj</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># compute degree</span>
        <span class="n">degree</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># GCN equation</span>
        <span class="n">new_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bi,bij,bjk,kl-&gt;bil&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<p>上記のコードの大半はKeras/TFに固有のもので、変数を適切な場所に配置しています。ここで重要なのは2行だけです。1つめは、隣接行列の列について合計することでグラフの次数を計算する操作です：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">degree</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>2つめの重要な行は、GCN方程式 <a class="reference internal" href="#equation-gcn">(8.3)</a> を計算する部分です（ここでは活性化は省略しています）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">new_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bi,bij,bjk,kl-&gt;bil&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
<p>これで、いま実装したGCNレイヤーを試すことができるようになりました：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gcnlayer</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="c1"># we insert a batch axis here</span>
<span class="n">gcnlayer</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=
 array([[[0.        , 0.46567526, 0.07535715],
         [0.        , 0.12714943, 0.05325063],
         [0.01475453, 0.295794  , 0.39316285],
         [0.01475453, 0.295794  , 0.39316285],
         [0.01475453, 0.295794  , 0.39316285],
         [0.        , 0.38166213, 0.        ]]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=
 array([[[1., 1., 1., 1., 1., 0.],
         [1., 1., 0., 0., 0., 1.],
         [1., 0., 1., 0., 0., 0.],
         [1., 0., 0., 1., 0., 0.],
         [1., 0., 0., 0., 1., 0.],
         [0., 1., 0., 0., 0., 1.]]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<p>これにより (1) 新しいノード特徴、(2) 隣接行列が出力されます。このレイヤーを積み重ねて、GCNを複数回適用できることを確認しましょう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gcnlayer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=
array([[[0.        , 0.18908624, 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.145219  , 0.        ],
        [0.        , 0.        , 0.        ]]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=
array([[[1., 1., 1., 1., 1., 0.],
        [1., 1., 0., 0., 0., 1.],
        [1., 0., 1., 0., 0., 0.],
        [1., 0., 0., 1., 0., 0.],
        [1., 0., 0., 0., 1., 0.],
        [0., 1., 0., 0., 0., 1.]]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<p>うまくいきました！しかし、なぜゼロの値があるのでしょうか？これはおそらく、出力に負の値が含まれており、それがReLU活性化を通した際に0になったためでしょう。これはモデルの訓練が不十分なために起きていると考えられ、訓練を重ねることで解決するでしょう</p>
</section>
</section>
<section id="id13">
<h2><span class="section-number">8.5. </span>例：溶解度予測<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<p>次に、GCNによる溶解度の予測について説明します。以前に、分子データセットに含まれている特徴量を使って予測モデルを組んだことを思い出してください。いま我々はGCNを使えるようになったので、特徴量に頭を悩ますことなく、分子構造を直接ニューラルネットワークに入力できるようになりました。GCNレイヤーは各ノードについての特徴を出力しますが、溶解度を予測するためには、グラフ全体についての特徴を得る必要があります。このプロセスをさらに洗練する方法については後で説明しますが、ここでは、GCNレイヤー後のすべてのノード特徴の平均を使うことにします。これにより単純かつ順序不変に、ノード特徴をグラフ特徴に変換することができます。この実装は次のとおりです：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GRLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A GNN layer that computes average over all node features&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;GRLayer&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduction</span>
</pre></div>
</div>
</div>
</div>
<p>上のコードで重要な点は、ノードについて平均をとっている(<code class="docutils literal notranslate"><span class="pre">axis=1</span></code>)次の部分だけです：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>この溶解度予測器を完成させるため、いくつかの全結合層を追加して、回帰が行えることを確認しましょう。回帰の場合は最終層の出力がそのまま予測結果となるため、最終層には活性化を適用しないことに注意してください。このモデルは <a class="reference external" href="https://keras.io/guides/functional_api/">Keras functional API</a> を使って実装されています。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ninput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="mi">100</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">ainput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="c1"># GCN block</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)([</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># reduce to graph features</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GRLayer</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># standard layers (the readout)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>この100はどこから来たのでしょうか？その答えはデータセットに含まれる元素の数にあります。このデータセットは多数の元素を含むため、以前使ったサイズ3のone-hot encodingでは、全ての元素を表現できません。前回はC, H, Oの元素さえ表現できれば十分でしたが、今回はより多数の元素を扱う必要があります。そのため、one-hot encodingのサイズも100に増やすことにしました。これで最大100種類の元素を表現できます。この拡張のために、モデルだけでなく<code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code>関数も更新することにしましょう。</p>
<div class="cell tag_hidden-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">()]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span> <span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;Ignoring bond order&quot;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">adj</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="s2">&quot;CO&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.0107595]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>GCNではエッジ特徴が使えないので、隣接テンソルの代わりに行列を使うよう変更を加えました。</p>
</aside>
<p>このモデルは1つの数値（スカラー）を出力します。
さて、学習可能なデータセットを得るために、いくつか作業が必要です。このデータセットは少し複雑で、特徴はテンソル(<span class="math notranslate nohighlight">\(mathbf{V}, \mathbf{E}\)</span>)のタプルなので、データセットは次のようなタプルのタプルになります： <span class="math notranslate nohighlight">\(\left((\mathbf{V}, \mathbf{E}), y\right)\)</span> <strong>generator</strong>はPythonの関数で、値を繰り返し返すことができます。ここでは、学習データを1つずつ取り出すためにgeneratorを使います。続いて、これを <code class="docutils literal notranslate"><span class="pre">from_generator</span></code> <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" title="(in TensorFlow v2.8)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code></a> コンストラクタに渡します。このコンストラクタでは、入力データのshapeを明示的に指定する必要があります。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)):</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="n">soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">sol</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">Solubility</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">graph</span><span class="p">,</span> <span class="n">sol</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
    <span class="n">example</span><span class="p">,</span>
    <span class="n">output_types</span><span class="o">=</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">output_shapes</span><span class="o">=</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([]),</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>ここまで来たらもう少しです。これで、いつものようにデータセットをtrain/val/testに分割できます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>そして、いよいよモデルの訓練です</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_39_0.png" src="../_images/gnn_39_0.png" />
</div>
</div>
<p>このモデルは明らかにアンダーフィットです。考えられる理由の一つはバッチサイズが1であることです。このモデルでは、原子個数を可変にした副作用としてバッチサイズを1より大きくできない制約があります。もう少し詳しく説明すると、任意のバッチサイズを入力できるようにすると、原子個数とバッチサイズの2つが不定となります。Keras/tensorflowは入力データのshapeに未知の次元が2つ以上ある場合データを処理できないため、バッチサイズを1に固定することで対処しています。ここでは扱いませんが、この問題を回避してバッチサイズを1より大きくするための標準的トリックは、複数の分子を（分子間の結合がない）1つのグラフにまとめてしまうことです。これによりデータの次元はそのままに、分子をバッチ処理することができます。</p>
<p>それではパリティプロットで予測精度を確認してみましょう。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;correlation = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="nb">min</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;loss = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">test_y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Testing Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_41_0.png" src="../_images/gnn_41_0.png" />
</div>
</div>
</section>
<section id="message-passinggcn">
<h2><span class="section-number">8.6. </span>Message PassingとGCN<a class="headerlink" href="#message-passinggcn" title="Permalink to this headline">¶</a></h2>
<p>より広い意味でGCNレイヤーを捉えると、GCNレイヤーは”message-passing”レイヤーの一つと言えます。GCNでは、まず近傍ノードからやってくるメッセージを処理します：</p>
<div class="amsmath math notranslate nohighlight" id="equation-49d82e17-a649-489d-b984-a16d307ed956">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-49d82e17-a649-489d-b984-a16d307ed956" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{{s_i}j} = \vec{v}_{{s_i}j} \mathbf{W}
\end{equation}\]</div>
<p>ここで <span class="math notranslate nohighlight">\(v_{{s_i}j}\)</span> は ノード <span class="math notranslate nohighlight">\(i\)</span> の <span class="math notranslate nohighlight">\(j\)</span> 番目の近傍です。 <span class="math notranslate nohighlight">\(s_i\)</span> は <span class="math notranslate nohighlight">\(i\)</span> に対するセンダー（送信者）です。 これはGCNがどのようにメッセージを計算するか示したものですが、やっていることは単純で、各近傍ノードの特徴に重み行列をかけているだけです。ノード <span class="math notranslate nohighlight">\(i\)</span> に向かうメッセージ <span class="math notranslate nohighlight">\(\vec{e}_{{s_i}j}\)</span> を得た後、これらのメッセージをノードの順番に対して不変な関数を用いて集約します：</p>
<div class="amsmath math notranslate nohighlight" id="equation-c03ab854-0ba2-4c66-9a5a-33aa917c7202">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-c03ab854-0ba2-4c66-9a5a-33aa917c7202" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{i} = \frac{1}{|\vec{e}_{{s_i}j}|}\sum \vec{e}_{{s_i}j} 
\end{equation}\]</div>
<p>上で扱ったように、GCNではこの集約は単なる平均ですが、任意の（例えば学習可能な）順序不変の関数を使うこともできます：</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>訳注：「学習可能な関数」という表現が時々出てきますが、これはtrainable functionの訳で「その関数自体を（具体的な関数形を明示的に記述するのではなく）MLP等の機械学習モデルとデータによりモデリングすることで、所望の性質を備えた関数を構成する」というアプローチにより構成された関数を指しています。</p>
</aside>
<div class="amsmath math notranslate nohighlight" id="equation-a0d6bf16-039c-42fb-8770-493f4dc85f9d">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-a0d6bf16-039c-42fb-8770-493f4dc85f9d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_{i} = \sigma(\vec{e}_i)
\end{equation}\]</div>
<p><span class="math notranslate nohighlight">\(v^{'}\)</span> は新しいノード特徴を示しています。これは単純に、集約された後で活性化関数を適用したメッセージです。このように書き出すことで、これらの手順にいくらでも小さな変更が加えられることに気づいたのではないでしょうか。Gilmerらによる重要な論文 <span id="id14">[<a class="reference internal" href="#id81" title="Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.">GSR+17</a>]</span> では、いくつかの選択肢を検討し、このメッセージパッシングレイヤーの基本的なアイディアが、量子力学に基づいて分子エネルギーを予測するタスクでうまくいくことが述べられています。GCN式に変更を加えた例としては、近傍メッセージの計算においてエッジ特徴を含めたり、単に <span class="math notranslate nohighlight">\(\sigma\)</span> で和をとる代わりに全結合層を使うといった試みが挙げられます。
これらから、GCNは、メッセージパッシンググラフニューラルネットワーク（MPNNと略されることもあります）の一種と考えることができます。</p>
</section>
<section id="gated-graph-neural-network">
<h2><span class="section-number">8.7. </span>Gated Graph Neural Network<a class="headerlink" href="#gated-graph-neural-network" title="Permalink to this headline">¶</a></h2>
<p>メッセージパッシングレイヤーの最も有名な亜種の一つは、 <strong>gated graph neural network</strong> (GGN) <span id="id15">[<a class="reference internal" href="#id79" title="Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.">LTBZ15</a>]</span> です。これは最後の式のノード更新を次で置き換えたものです：</p>
<div class="amsmath math notranslate nohighlight" id="equation-5598e065-a4e0-4b42-93fa-229539f86ba1">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-5598e065-a4e0-4b42-93fa-229539f86ba1" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_{i} = \textrm{GRU}(\vec{v}_i, \vec{e}_i)
\end{equation}\]</div>
<p><span class="math notranslate nohighlight">\(\textrm{GRU}(\cdot, \cdot)\)</span> はゲート再帰ユニット（gated recurrent unit）<span id="id16">[<a class="reference internal" href="#id80" title="Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.">CGCB14</a>]</span> です。 GRU はバイナリ（入力引数を2つ持つ）ニューラルネットワークで、典型的には系列データのモデリングに使用されます。GCNと比較して、GGNの興味深い特徴は、（GRUからの）ノード更新において学習可能なパラメータを持つことで、より柔軟性を備えたモデルとなっていることです。GGNでは、GRUのパラメータは各層で共有されます（GRUを使って系列データをモデリングする方法と同じです）。パラメータ共有によるメリットは、学習すべきパラメータを増やすことなくGGNの層を無限に積み重ねられることです（各層で <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> を揃えることが前提です）。このため、GGNは大きなタンパク質や大きなユニットセルをもつ結晶構造のような、大きなグラフに適しています（訳注：GGNレイヤーを何層もスタックすることで、より遠くのノードからの情報を取り込むことができるためです）。</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>GNNにはよく”gated”という接頭語がついていますが、これはノードがGRUによって更新されることを意味します。</p>
</aside>
</section>
<section id="pooling">
<h2><span class="section-number">8.8. </span>Pooling<a class="headerlink" href="#pooling" title="Permalink to this headline">¶</a></h2>
<p>メッセージパッシングの観点、および一般にGNNSでは、近傍からのメッセージを結合する方法が重要なステップとなります。このステップは畳み込みニューラルネットワークで使われるプーリング層に似ているため、<strong>プーリング</strong>と呼ばれることもあります。畳み込みニューラルネットワークのプーリングと同じように、このために使用できる縮約操作は複数あります。一般的に、GNNのプーリングにはメッセージの合計または平均が使われますが、グラフ同型ネットワーク（Graph Isomorphism Networks） <span id="id17">[<a class="reference internal" href="#id94" title="Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations. 2018.">XHLJ18</a>]</span> のように非常に洗練された操作を用いることもできます。この他にも、例えば注意（attention）の章では自己注意（self-attention）を使う例を扱いますが、これらの操作もプーリングに使えます。プーリングのステップを色々と工夫したくなることもありますが、プーリング操作の選択はモデルの性能にとってそれほど重要ではないことが経験的に分かっています <span id="id18">[<a class="reference internal" href="#id93" title="Enxhell Luzhnica, Ben Day, and Pietro Liò. On graph classification networks, datasets and baselines. arXiv preprint arXiv:1905.04682, 2019.">LDLio19</a>, <a class="reference internal" href="#id92" title="Diego Mesquita, Amauri Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. Advances in Neural Information Processing Systems, 2020.">MSK20</a>]</span>。プーリングの重要な特性は<em>順序不変性</em>で、集約操作はノード（プーリングの場合はエッジ）の順序に依存しない性質を備えることが望まれます。Grattarolaら <span id="id19">[<a class="reference internal" href="#id199" title="Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding pooling in graph neural networks. arXiv preprint arXiv:2110.05292, 2021.">GZBA21</a>]</span>が、プーリング手法に関する最近のレビューを出版しています。</p>
<p>Daigavaneらの論文 <span id="id20">[<a class="reference internal" href="#id201" title="Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal. Understanding convolutions on graphs. Distill, 2021. https://distill.pub/2021/understanding-gnns. doi:10.23915/distill.00032.">DRA21</a>]</span> では、様々なプーリング戦略の比較と概要が視覚的に紹介されています。</p>
</section>
<section id="readout-function">
<h2><span class="section-number">8.9. </span>Readout Function<a class="headerlink" href="#readout-function" title="Permalink to this headline">¶</a></h2>
<p>GNNの出力はグラフです（そう設計されているので当たり前ですが）。しかし、予測したいラベルもグラフであることは稀で、一般的にはラベルは各ノードまたはグラフ全体に対して付与されています。ノードラベルの例は原子の部分電荷、グラフラベルの例は分子のエネルギーです。GNNから出力されるグラフを、予測ターゲットであるノードラベルやグラフラベルに変換するプロセスを <strong>読み出し （readout）</strong> と呼びます。ノードラベルを予測する場合であれば、単純にエッジ特徴を捨てて、GNNから出力されるノード特徴ベクトルを予測結果として扱うことができます。この場合、出力層の前にいくつかの全結合層を挟むことが多いでしょう。</p>
<p>分子のエネルギーや実効電荷のようなグラフレベルのラベルを予測する場合、ノード/エッジ特徴をグラフラベルに変換するプロセスに注意が必要です。所望のshapeのグラフラベルを得るために、単純にノード特徴を全結合層に入力した場合、順序等価性が失われてしまいます（出力はノードラベルではなくグラフラベルなので、厳密には、順序等価性ではなく順序不変性です）。溶解度の例で用いた読み出しは、ノード特徴量に対して縮約操作を行うことでした。その後でグラフ特徴を全結合層に入力して予測結果を得ました。実は、これがグラフ特徴の読み出しを行う唯一の方法であることが示されています <span id="id21">[<a class="reference internal" href="#id119" title="Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, 3391–3401. 2017.">ZKR+17</a>]</span> 。すなわち、グラフ特徴を得るためにノード特徴の縮約操作を行い、このグラフ特徴を全結合層に入力することで予測結果であるグラフラベルを得ます。各ノードの特徴量に対してそれぞれ全結合層を通す操作もできますが、ノード特徴への全結合層の適用はGNN内部で既に行われているので、あまりお勧めしません。このグラフ特徴の読み出しはDeepSetsと呼ばれることもあります。これは、特徴が集合（訳注：順序を持たず、個数が不定）として与えられる場合のために設計された、順序不変なアーキテクチャであるDeepSets <span id="id22">[<a class="reference internal" href="#id119" title="Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, 3391–3401. 2017.">ZKR+17</a>]</span> と同じ形であるためです。</p>
<p>プーリングも読み出しも順序不変の関数が使われていることにお気づきでしょうか。したがって、DeepSetsはプーリングに、attentionは読み出しに使用することもできます。</p>
<section id="intensive-vs-extensive">
<h3><span class="section-number">8.9.1. </span>Intensive vs Extensive<a class="headerlink" href="#intensive-vs-extensive" title="Permalink to this headline">¶</a></h3>
<p>回帰タスクでの読み出しにおいて考慮すべき重要な点の1つは、ラベルが <strong>intensive</strong> か <strong>extensive</strong> かです。Intensiveラベルは、ノード（＝原子）の数に依存しない値を持つラベルです。例えば、屈折率や溶解度などはIntensiveラベルです。Intensiveラベルの読み出しは、一般にノードの数に依存しないことが要請されます。したがって、この場合の読み出しにおける縮約操作として、平均や最大をとる操作は適用可能ですが、ノード数により値が変わるため合計は適しません。対照的に、Extensiveラベルでは、（一般的には）読み出しの縮約操作には（ノード数を反映できるため）合計が適します。extensiveな分子特性の例には、生成エンタルピーが挙げられます。</p>
</section>
</section>
<section id="battaglia-general-equations">
<h2><span class="section-number">8.10. </span>Battaglia General Equations<a class="headerlink" href="#battaglia-general-equations" title="Permalink to this headline">¶</a></h2>
<p>ここまでで学んだように、GNNレイヤーはメッセージパッシングレイヤーとして一般化することができました。Battagliaら <span id="id23">[<a class="reference internal" href="#id78" title="Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.">BHB+18</a>]</span> はさらに進んで、ほぼすべてのGNNを記述できる一般的な方程式の集合を考案しました。彼らはGNNレイヤーの方程式を、メッセージパッシングレイヤーの方程式におけるノード更新式のような3つの更新方程式と、3つの集約方程式という合計6つの式に分解しました。これらの式では、グラフ特徴ベクトルという新しい概念が導入されています。このアイディアでは、ネットワークに2つの部分（GNNと読み出し）を持たせる代わりに、グラフレベルの特徴を各GNNレイヤーで更新するアプローチをとります。グラフ特徴ベクトルは、グラフ全体を表す特徴の集合です。例えば溶解度を計算する場合は、読み出し関数を持つ代わりに分子全体の特徴ベクトルを構築し、これを更新して最終的に溶解度を予測する方法が有効だった可能性もあります。このように、分子全体について定義されるあらゆる種類の量（例：溶解度、エネルギー）は、グラフレベルの特徴ベクトルを用いて予測できるでしょう。</p>
<p>これらの式の最初のステップはエッジの特徴ベクトルの更新であり、新たに導入する変数である <span class="math notranslate nohighlight">\(\vec{e}_k\)</span> についての式として記述されます：</p>
<div class="math notranslate nohighlight" id="equation-edge-update">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-edge-update" title="Permalink to this equation">¶</a></span>\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right)
\]</div>
<p><span class="math notranslate nohighlight">\(\vec{e}_k\)</span> はエッジ <span class="math notranslate nohighlight">\(k\)</span>　の特徴ベクトル、 <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> はエッジ <span class="math notranslate nohighlight">\(k\)</span> について受信されたノード特徴ベクトル、 <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> はエッジ <span class="math notranslate nohighlight">\(k\)</span> についてノード特徴ベクトルを送信したノード、 <span class="math notranslate nohighlight">\(\vec{u}\)</span> はグラフ特徴ベクトル、 <span class="math notranslate nohighlight">\(\phi^e\)</span> はGNNレイヤーの定義に使われる3種類の更新関数のうちの一つです。ただし、ここで言う3種類の更新関数とは一般化した表現であり、必ずしも3種類を定義する必要はありません。ここではそのうちの一つである <span class="math notranslate nohighlight">\(\phi^e\)</span> を用いてGNNレイヤーを定義します。</p>
<p>ここで扱う分子グラフは無向グラフなので、どのノードが <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> を受信し、どのノードが <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> を送信するかをどのように決めれば良いでしょうか？ それぞれの <span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> は次のステップでノード <span class="math notranslate nohighlight">\(v_{rk}\)</span> への入力として集約されます。分子グラフでは、全ての結合は原子からの「入力」と「出力」の両方を兼ねるため、全ての結合を2つの有向エッジとして取り扱うことにします（他に良い方法がないのです）：C-H結合はCからHへの辺とHからCへのエッジで構成されることになります。最初の疑問に戻りますが、 <span class="math notranslate nohighlight">\(\vec{v}_{rk} \)</span>と <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> とは何でしょうか？隣接行列のすべての要素（<span class="math notranslate nohighlight">\(k\)</span>）を考え、<span class="math notranslate nohighlight">\(k = \{ij\}\)</span> すなわち要素 <span class="math notranslate nohighlight">\(A_{ij}\)</span> については、受信ノードが <span class="math notranslate nohighlight">\(j\)</span> 、送信ノードが <span class="math notranslate nohighlight">\(i\)</span> であることを表します。逆向きのエッジにおける隣接行列の要素 <span class="math notranslate nohighlight">\(A_{ji}\)</span> を考えると、受信ノードが <span class="math notranslate nohighlight">\(i\)</span> 、送信ノードが <span class="math notranslate nohighlight">\(j\)</span> となります。</p>
<p><span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> はGCNからのメッセージのようなものですが、より一般的で、受信ノードとグラフ特徴ベクトル <span class="math notranslate nohighlight">\(\vec{u}\)</span> の情報を反映することができます。日常的な意味での「メッセージ」は一度送信されれば誰（あるいは何）に受信されるかによって内容が変わるわけではないので、 <span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> をメッセージの比喩で説明しようとするとおかしなことになります。ともかく、新しいエッジの更新は、最初の集約関数で集約されます：</p>
<div class="math notranslate nohighlight" id="equation-edge-aggregation">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-edge-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right)
\]</div>
<p><span class="math notranslate nohighlight">\(\rho^{e\rightarrow v}\)</span> は我々が定義した関数、 <span class="math notranslate nohighlight">\(E_i^{'}\)</span> はノード <span class="math notranslate nohighlight">\(i\)</span> に <strong>向かう</strong> 全てのエッジからの <span class="math notranslate nohighlight">\(\vec{e}^{'}_k\)</span> をスタックしたものです。集約されたエッジを使って、ノードの更新を計算できます：</p>
<div class="math notranslate nohighlight" id="equation-node-update">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-node-update" title="Permalink to this equation">¶</a></span>\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right)
\]</div>
<p>以上で新しいノードとエッジが得られたので、GNNレイヤーの通常のステップは完了です。もしグラフ特徴 (<span class="math notranslate nohighlight">\(\vec{u}\)</span>) を更新する場合、以下のステップが追加で定義されることがあります：</p>
<div class="math notranslate nohighlight" id="equation-edge-all-aggregation">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-edge-all-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{e}^{'} = \rho^{e\rightarrow u}\left( E^{'}\right)
\]</div>
<p>この式は、グラフ全体について全てのメッセージ/集約されたエッジを集約します。これにより、新しいノードをグラフ全体について集約できます：</p>
<div class="math notranslate nohighlight" id="equation-node-all-aggregation">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-node-all-aggregation" title="Permalink to this equation">¶</a></span>\[
\bar{v}^{'} = \rho^{v\rightarrow u}\left( V^{'}\right)
\]</div>
<p>そして最後に、次のようにしてグラフ特徴ベクトルを更新できます：</p>
<div class="math notranslate nohighlight" id="equation-global-update">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-global-update" title="Permalink to this equation">¶</a></span>\[
\vec{u}^{'} = \phi^u\left( \bar{e}^{'},\bar{v}^{'}, \vec{u}\right)
\]</div>
<section id="battaglia-equationsgcn">
<h3><span class="section-number">8.10.1. </span>Battaglia equationsによるGCNの再定式化<a class="headerlink" href="#battaglia-equationsgcn" title="Permalink to this headline">¶</a></h3>
<p>Battagliaの式によりGCNがどのように記述されるか見てみましょう。まず <a class="reference internal" href="#equation-edge-update">(8.8)</a> を使って、隣接する可能性のあるすべての隣接ノードに対してメッセージを計算します。GCNでは、メッセージは送信者にのみ依存し、受信者には依らないことに注意してください。</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>ここで”エッジの更新”関数を使っていますが、GCNではエッジの特徴を無視していることに注意してください。ここでは、エッジはグラフの接続を定義するためにのみ使われています。</p>
</aside>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right) = \vec{v}_{sk} \mathbf{W}
\]</div>
<p><a class="reference internal" href="#equation-edge-aggregation">(8.9)</a> においてノード <span class="math notranslate nohighlight">\(i\)</span> にやってくるメッセージを集約するために、これらのメッセージの平均をとります：</p>
<div class="math notranslate nohighlight">
\[
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right) = \frac{1}{|E_i^{'}|}\sum E_i^{'}
\]</div>
<p>続いて、ノードの更新を行いますが、これは単にメッセージについて活性化関数を適用するだけです <a class="reference internal" href="#equation-node-update">(8.10)</a></p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right) = \sigma(\bar{e}^{'}_i)
\]</div>
<p>上式において <span class="math notranslate nohighlight">\(\sigma(\bar{e}^{'}_i + \vec{v}_i)\)</span> と変更を加えることで、グラフに自己ループを持たせることも可能です。GCNでは他の関数は必要ないので、これら3つの式だけでGCNを定義することができます。</p>
</section>
</section>
<section id="the-schnet-architecture">
<h2><span class="section-number">8.11. </span>The SchNet Architecture<a class="headerlink" href="#the-schnet-architecture" title="Permalink to this headline">¶</a></h2>
<p>最も古く、かつよく用いられるGNNの1つに、SchNetネットワーク <span id="id24">[<a class="reference internal" href="#id227" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span> があります。発表当時はあまりGNNとしては認識されていませんでしたが、現在ではその一つとして認識され、ベースラインモデルとしてよく使われています。<strong>ベースライン</strong>モデルとは、新手法との比較に用いられるモデルのことで、広く受け入れられ、かつ様々な実験を通じて良い性能を示すことが確認されているモデルが使われます。</p>
<aside class="margin sidebar">
<p class="sidebar-title">ベースラインモデルについて</p>
<p>Tipsとして、ディープラーニングで実際の問題を解決したいのであれば、その分野の最新の人気論文において提案されたモデルではなく、その論文で比較に用いられているベースラインモデルを使うべきだというものがあります。この理由は、ベースラインモデルは、最も正確であることよりも、簡単で、速く、よくテストされていることが重要であるため、こうした性質が実問題の解決にも重要であるためです。</p>
</aside>
<p>これまでに扱った全ての例では、分子をグラフとしてモデルへ入力していました。一方SchNetでは、分子グラフではなく、原子をxyz座標（点）として表現し入力し、xyz座標をグラフに変換することでGNNNを適用します。SchNetは、結合情報なしで原子の配置のみからエネルギーや力を予測するために開発されました。したがって、SchNetを理解するために、まず原子とその位置のセットがどのようにグラフに変換されるかを確認しましょう。各原子をノード化する手順は簡単で、上記と同様の処理を行った後、原子番号をembeddingレイヤーに渡します。これは、各原子番号に学習可能なベクトルを割り当てること（訳注：学習により、各原子の抽象的特徴を捉えた高次元ベクトルを得ること）を意味します（embeddingについての復習は <a class="reference internal" href="layers.html"><span class="doc">Standard Layers</span></a> を参照してください）。</p>
<p>隣接行列の計算も簡単で、全ての原子が全ての原子に接続されるようにするだけです。単に全原子が相互に接続するのだとしたら、GNNを使う意味がよくわからないと思われるかもしれません。このような操作をする理由は、GNNは順序等価であるからです。もし原子をxyz座標として学習しようとすると，原子の並び方によって重みが変わってしまう上に、構造ごとの原子数の違いをうまく取り扱えないことでしょう。</p>
<p>SchNetを理解するためにもう一つ押さえるべき点は、各原子のxyz座標の情報はどう扱われるのか、ということです。SchNetでは、xyz座標からエッジ特徴を構築することにより、モデルに座標の情報を取り込んでいます。原子 <span class="math notranslate nohighlight">\(i\)</span> と <span class="math notranslate nohighlight">\(j\)</span> の間のエッジ <span class="math notranslate nohighlight">\(\vec{e}\)</span> は、シンプルにこれらの原子間距離 <span class="math notranslate nohighlight">\(r\)</span> から計算されます。</p>
<div class="math notranslate nohighlight" id="equation-rbf-edge">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-rbf-edge" title="Permalink to this equation">¶</a></span>\[
e_k = \exp\left(-\gamma \left(r - \mu_k\right)^2\right)
\]</div>
<p><span class="math notranslate nohighlight">\(\gamma\)</span> はハイパーパラメータ（例： 10Å） <span class="math notranslate nohighlight">\(\mu_k\)</span> は <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">5,</span> <span class="pre">10,</span> <span class="pre">15</span> <span class="pre">,</span> <span class="pre">20]</span></code> のようなスカラーの等間隔グリッドです。 <a class="reference internal" href="#equation-rbf-edge">(8.14)</a> の操作は、原子番号や共有結合の種類のようなカテゴリ特徴をone-hotベクトルに変換することに似ています。しかし、カテゴリカルな量と異なり、距離は連続値で無限にあるので、one-hotベクトルとして表現することはできません。そこで、一種の「スムージング」によって、距離を擬似的にone-hot表現しているのです。この感覚をつかむために、例を見てみましょう：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">rbf</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input 2
output [0.02 0.78 0.   0.   0.  ]
</pre></div>
</div>
</div>
</div>
<p>距離　<span class="math notranslate nohighlight">\(r=2\)</span> は、 <span class="math notranslate nohighlight">\(k = 1\)</span>　の位置（訳注：kはこのlistのindexのことです）が強く活性化したベクトルを与えることがわかります。これは <span class="math notranslate nohighlight">\(\mu_1 = 2\)</span> であることに対応します。</p>
<p>ここまででノードとエッジおよび、GNNの更新方程式を定義しました。さらにもう少しだけ記号を定義しておく必要があります。ここでは、MLP（Multilayer perceptron）を表すために <span class="math notranslate nohighlight">\(h(\vec{x})\)</span> を使用します。MLPは、基本的に1層あるいは2層の全結合レイヤーからなるニューラルネットワークです。これらのMLPにおける全結合レイヤーの正確な数や、いつ・どこで活性化を行うかといった詳細は、重要な点を理解する上で不要なため説明は省略します（これらの詳細は以下の実装例を参照してください）。ではここで、全結合レイヤーの定義を思い出しましょう：</p>
<div class="math notranslate nohighlight">
\[
h(\vec{x}) = \sigma\left(Wx + b\right)
\]</div>
<p>また、SchNetでは “shifted softplus” と呼ばれる新たな活性化関数 <span class="math notranslate nohighlight">\(\sigma\)</span> を用います： <span class="math notranslate nohighlight">\(\sigma = \ln\left(0.5e^{x} + 0.5\right)\)</span> 。 <a class="reference internal" href="#softplus"><span class="std std-numref">Fig. 8.4</span></a> において、 <span class="math notranslate nohighlight">\(\sigma(x)\)</span> と通常のReLU活性化を比較した分析が報告されています。shifted softplusを使う理由は、入力に対して滑らかであるためです。このため、粒子同士の距離（pairwise distance）に対して滑らかな微分が必要とされる分子動力学シミュレーションのようなアプリケーションにおいて、フォース（訳注：粒子同士に働く力）を計算するためにSchNetを使うことができます。</p>
<figure class="align-default" id="softplus">
<div class="cell_output docutils container">
<img alt="../_images/gnn_49_0.png" src="../_images/gnn_49_0.png" />
</div>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">一般的なReLU活性化関数と、SchNetで使用されているshifted softplusの比較</span><a class="headerlink" href="#softplus" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>さて、前置きが続きましたが、ようやくGNN方程式に話を移します。エッジの更新方程式 <a class="reference internal" href="#equation-edge-update">(8.8)</a> は2つの部分から成ります。まず、やってくるエッジ（＝結合）特徴と、ノード（＝原子）の特徴をMLPに通します。続いて、それらの結果を次のMLPに通します：</p>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right) =h_1\left(\vec{v}_{sk}\right) \cdot h_2\left(\vec{e}_k\right)
\]</div>
<p>次にエッジの集約関数 <a class="reference internal" href="#equation-edge-aggregation">(8.9)</a> を考えましょう。SchNetでは、エッジ集約は近傍の原子特徴量に対する和です。</p>
<div class="math notranslate nohighlight">
\[
\bar{e}^{'}_i = \sum E_i^{'}
\]</div>
<p>最後に、SchNetのノード更新関数は以下のようになります：</p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right) = \vec{v}_i + h_3\left(\bar{e}^{'}_i\right)
\]</div>
<p>通常、GNNの更新は3〜6回適用されます。上記のSchNetの説明において、エッジの更新式を定義しましたが、GCN同様に実際にはエッジ特徴を上書きせず、各層で同じエッジ特徴が保たれます。元々のSchNetはエネルギーや力を予測するためのものなので、読み出しはsum-poolingや上記のような戦略で行うことが可能です。</p>
<p>これらの式の詳細は変更されることもありますが、オリジナルのSchNetの論文では、 <span class="math notranslate nohighlight">\(h_1\)</span> は活性化なしのdenseな1層、<span class="math notranslate nohighlight">\(h_2\)</span> は活性化ありの2層、<span class="math notranslate nohighlight">\(h_3\)</span> は1層に活性化あり・2層目は活性化無しのdenseな2層の構成が用いられました。</p>
<div class="admonition-schnet admonition">
<p class="admonition-title">SchNetとは?</p>
<p>SchNetベースのGNNの主な特徴は、（1）エッジの更新（メッセージの組み立て）にエッジとノードの特徴を用いることです：</p>
<div class="math notranslate nohighlight">
\[
\vec{e}^{'}_k = h_1(\vec{v}_{sk}) \cdot h_2(\vec{e}_k)
\]</div>
<p>ここで <span class="math notranslate nohighlight">\(h_i()\)</span> は何らかの学習可能な関数です。特徴（2）は、ノード更新に残差を利用することです：</p>
<div class="math notranslate nohighlight">
\[
\vec{v}^{'}_i = \vec{v}_i + h_3\left(\bar{e}^{'}_i\right)
\]</div>
</div>
<p>その他、エッジ特徴の作り方、<span class="math notranslate nohighlight">\(h_i\)</span> の層数、活性化関数の選択、読み出しの方法、ポイントクラウドをグラフに変換する方法など、詳細は全て <span id="id25">[<a class="reference internal" href="#id227" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span> で提案されたSchNetモデルの定義に準拠します。</p>
</section>
<section id="schnet-example-predicting-space-groups">
<h2><span class="section-number">8.12. </span>SchNet Example: Predicting Space Groups<a class="headerlink" href="#schnet-example-predicting-space-groups" title="Permalink to this headline">¶</a></h2>
<p>Our next example will be a SchNet model that predict space groups of points. Identifying the space group of atoms is an important part of crystal structure identification, and when doing simulations of crystallization. Our SchNet model will take as input points and output the predicted space group. This is a classification problem; specifically it is multi-class becase a set of points should only be in one space group. To simplify our plots and analysis, we will work in 2D where there are 17 possible space groups.</p>
<p>Our data for this is a set of points from various point groups. The features are xyz coordinates and the label is the space group. We will not have multiple atom types for this problem. The hidden cell below loads the data and reshapes it for the example.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">urllib</span>

<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/whitead/dmol-book/raw/master/data/sym_trajs.pb.gz&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sym_trajs.pb.gz&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;sym_trajs.pb.gz&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">trajs</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">label_str</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">trajs</span><span class="p">]))</span>

<span class="c1"># now build dataset</span>
<span class="k">def</span> <span class="nf">generator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">trajs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ls</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label_str</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>
        <span class="n">traj</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">yield</span> <span class="n">traj</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
    <span class="n">generator</span><span class="p">,</span>
    <span class="n">output_signature</span><span class="o">=</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># The shuffling above is really important because this dataset is in order of labels!</span>

<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s take a look at a few examples from the dataset</p>
<div class="dropdown admonition">
<p class="admonition-title">The Data</p>
<p>This data was generated from <span id="id26">[<a class="reference internal" href="#id228" title="Sam Cox and Andrew D White. Symmetric molecular dynamics. arXiv preprint arXiv:2204.01114, 2022.">CW22</a>]</span> and all points are constrained to match the space group exactly during a molecular dynamics simulation. The trajectories were NPT with a positive pressure and followed the procedure in that paper for Figure 2. The force field is Lennard-Jones with <span class="math notranslate nohighlight">\(\sigma=1\)</span> and <span class="math notranslate nohighlight">\(\epsilon=1\)</span></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># get a few example and plot them</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_55_0.png" src="../_images/gnn_55_0.png" />
</div>
</div>
<p>You can see that there is a variable number of points and a few examples for each space group. The goal is to infer those titles on the plot from the points alone.</p>
<section id="building-the-graphs">
<h3><span class="section-number">8.12.1. </span>Building the graphs<a class="headerlink" href="#building-the-graphs" title="Permalink to this headline">¶</a></h3>
<p>We now need to build the graphs for the points. The nodes are all identical - so they can just be 1s (we’ll reserve 0 in case we want to mask or pad at some point in the future). As described in the SchNet section above, the edges should be distance to every other atom. In most implementations of SchNet, we practically add a cut-off on either distance or maximum degree (edges per node). We’ll do maximum degree for this work of 16.</p>
<p>I have a function below that is a bit sophisticated. It takes a matrix of point positions in arbitrary dimension and returns the distances and indices to the nearest <code class="docutils literal notranslate"><span class="pre">k</span></code> neighbors - exactly what we need. It uses some tricks from <a class="reference internal" href="../math/tensors-and-shapes.html"><span class="doc">Tensors and Shapes</span></a>. However, it is not so important for you to understand this function. Just know it takes in points and gives us the edge features and edge nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this decorator speeds up the function by &quot;compiling&quot; it (tracing it)</span>
<span class="c1"># to run efficienty</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">reduce_retracing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">get_edges</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">positions</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># adjust NN</span>
    <span class="n">NN</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">NN</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">qexpand</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># one column</span>
    <span class="n">qTexpand</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># one row</span>
    <span class="c1"># repeat it to make matrix of all positions</span>
    <span class="n">qtile</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">qexpand</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">qTtile</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">qTexpand</span><span class="p">,</span> <span class="p">[</span><span class="n">M</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c1"># subtract them to get distance matrix</span>
    <span class="n">dist_mat</span> <span class="o">=</span> <span class="n">qTtile</span> <span class="o">-</span> <span class="n">qtile</span>
    <span class="c1"># mask distance matrix to remove zros (self-interactions)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">dist_mat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">&gt;=</span> <span class="mf">5e-4</span>
    <span class="n">mask_cast</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># make masked things be really far</span>
    <span class="n">dist_mat_r</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">*</span> <span class="n">mask_cast</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask_cast</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="o">-</span><span class="n">dist_mat_r</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">NN</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">topk</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">topk</span><span class="o">.</span><span class="n">indices</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s see how this function works by showing the connections between points in one of our examples. I’ve hidden the code below. It shows some point’s neighbors and connects them so you can get a sense of how a set of points is converted into a graph. The complete graph will have all points’ neighborhoods.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">collections</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">e_f</span><span class="p">,</span> <span class="n">e_i</span> <span class="o">=</span> <span class="n">get_edges</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

    <span class="c1"># make things easier for plotting</span>
    <span class="n">e_i</span> <span class="o">=</span> <span class="n">e_i</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># make lines from origin to its neigbhors</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">23</span><span class="p">):</span>
        <span class="c1"># lines are [(xstart, ystart), (xend, yend)]</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">extend</span><span class="p">([[(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">])]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">e_i</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
        <span class="n">colors</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">e_i</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="n">lc</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">LineCollection</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">add_collection</span><span class="p">(</span><span class="n">lc</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_59_0.png" src="../_images/gnn_59_0.png" />
</div>
</div>
<p>We will now add this function and the edge featurization of SchNet <a class="reference internal" href="#equation-rbf-edge">(8.14)</a> to get the graphs for the GNN steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_DEGREE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">EDGE_FEATURES</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">MAX_R</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">MAX_R</span><span class="p">,</span> <span class="n">EDGE_FEATURES</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_graph</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">edge_r</span><span class="p">,</span> <span class="n">edge_i</span> <span class="o">=</span> <span class="n">get_edges</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">MAX_DEGREE</span><span class="p">)</span>
    <span class="n">edge_features</span> <span class="o">=</span> <span class="n">rbf</span><span class="p">(</span><span class="n">edge_r</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">edge_features</span><span class="p">,</span> <span class="n">edge_i</span><span class="p">),</span> <span class="n">y</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>


<span class="n">graph_train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
<span class="n">graph_val_data</span> <span class="o">=</span> <span class="n">val_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
<span class="n">graph_test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">make_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s examine one graph to see what it looks like. We’ll slice out only the first nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">nn</span><span class="p">),</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">graph_train_data</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node:&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node, first edge features:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;first node, all neighbors&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>first node: 1
first node, first edge features: [2.6120767e-01 5.5859104e-02 9.6979469e-10 1.3669308e-24 0.0000000e+00
 0.0000000e+00 0.0000000e+00 0.0000000e+00]
first node, all neighbors [283  15 262 266 302 199 277 204 212 112 269   9 201 192 275 259]
label [12]
</pre></div>
</div>
</div>
</div>
</section>
<section id="implementing-the-mlps">
<h3><span class="section-number">8.12.2. </span>Implementing the MLPs<a class="headerlink" href="#implementing-the-mlps" title="Permalink to this headline">¶</a></h3>
<p>Now we can implement the SchNet model! Let’s start with the <span class="math notranslate nohighlight">\(h_1,h_2,h_3\)</span> MLPs that are used in the GNN update equations. In the SchNet paper these each had different numbers of layers and different decisions about which layers had activation. Let’s create them now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ssp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># shifted softplus activation</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_h1</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">make_h2</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">make_h3</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">)]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One detail that can be missed is that the weights in each MLP should change in each layer of SchNet. Thus, we’ve written the functions above to always return a new MLP. This means that a new set of trainable weights is generated on each call, meaning there is no way we could erroneously have the same weights in multiple layers.</p>
</section>
<section id="implementing-the-gnn">
<h3><span class="section-number">8.12.3. </span>Implementing the GNN<a class="headerlink" href="#implementing-the-gnn" title="Permalink to this headline">¶</a></h3>
<p>Now we have all the pieces to make the GNN. This code will be very similar to the GCN example above, except we now have edge features. One more detail is that our readout will be an MLP as well, following the SchNet paper. The only change we’ll make is that we want our output property to be (1) multi-class classification and (2) intensive (independent of number of atoms). So we’ll end with an average (intensive) and end with an output vector of logits the size of our labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SchNetModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of SchNet Model&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gnn_blocks</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">label_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SchNetModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span> <span class="o">=</span> <span class="n">gnn_blocks</span>

        <span class="c1"># build our layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h1</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h2</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_h3</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">label_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">edge_features</span><span class="p">,</span> <span class="n">edge_i</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># turn node types as index to features</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gnn_blocks</span><span class="p">):</span>
            <span class="c1"># get the node features per edge</span>
            <span class="n">v_sk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">edge_i</span><span class="p">)</span>
            <span class="n">e_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">v_sk</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_features</span><span class="p">)</span>
            <span class="n">e_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">e_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">nodes</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">e_i</span><span class="p">)</span>
        <span class="c1"># readout now</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Remember that the key attributes of a SchNet GNN are the way that we use edge and node features. We can see the mixing of these two in the key line for computing the edge update (computing message values):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">e_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h1s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">v_sk</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_features</span><span class="p">)</span>
</pre></div>
</div>
<p>followed by aggregation of the edges updates (pooling messages):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">e_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">e_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>and the node update</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h3s</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">e_i</span><span class="p">)</span>
</pre></div>
</div>
<p>Also of note is how we go from node features to multi-classs. We use dense layers that get the shape per-node into the number of classes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">readout_l1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">ssp</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">readout_l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">label_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>and then we take the average over all nodes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Let’s give now use the model on some data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_schnet</span> <span class="o">=</span> <span class="n">SchNetModel</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_str</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">graph_train_data</span><span class="p">:</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.02455037  0.01222102 -0.01053292 -0.01197899  0.00917386  0.01241873
  0.03160297  0.02135343  0.00017726  0.00664034 -0.01682034 -0.01577481
  0.00546044 -0.00450091 -0.01376736  0.01938844  0.01023705]
</pre></div>
</div>
</div>
</div>
<p>The output is the correct shape and remember it is logits. To get a class prediction that sums to probability 1, we need to use a softmax:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predicted class&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predicted class [0.05999666 0.05926148 0.05792827 0.05784456 0.05908117 0.0592732
 0.06042128 0.05980515 0.05855203 0.05893168 0.05756519 0.05762541
 0.05886219 0.05827875 0.05774121 0.05968775 0.05914402]
</pre></div>
</div>
</div>
</div>
</section>
<section id="training">
<h3><span class="section-number">8.12.4. </span>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>Great! It is untrained though. Now we can set-up training. Our loss will be cross-entropy from logits, but we need to be careful on the form. Our labels are integers - which is called “sparse” labels because they are not full one-hots. Mult-class classification is also known as categorical classification. Thus, the loss we want is sparse categorical cross entropy from logits.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">small_schnet</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_accuracy&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">graph_train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">graph_val_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;sparse_categorical_accuracy&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_sparse_categorical_accuracy&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">17</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_75_0.png" src="../_images/gnn_75_0.png" />
</div>
</div>
<p>The accuracy is not great, but it looks like we could keep training. We have a very small SchNet here. Standard SchNet described in <span id="id27">[<a class="reference internal" href="#id227" title="Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24):241722, 2018.">SchuttSK+18</a>]</span> uses 6 layers and 64 channels and 300 edge features. We have 3 layers and 32 channels. Nevertheless, we’re able to get some learning. Let’s visually see what’s going on with the trained model on some test data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">gx</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">graph_test_data</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">20</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">small_schnet</span><span class="p">(</span><span class="n">gx</span><span class="p">)</span>
    <span class="n">yhat_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True: </span><span class="si">{</span><span class="n">label_str</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Predicted: </span><span class="si">{</span><span class="n">label_str</span><span class="p">[</span><span class="n">yhat_i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_77_0.png" src="../_images/gnn_77_0.png" />
</div>
</div>
<p>We’ll revisit this example later! One unique fact about this dataset is that it is <em>synthetic</em>, meaning there is no label noise. As discussed in <a class="reference internal" href="../ml/regression.html"><span class="doc">Regression &amp; Model Assessment</span></a>, that removes the possibility of overfitting and leads us to favor high variance models. The goal of teaching a model to predict space groups is to apply it on real simulations or microscopy data, which will certainly have noise. We could have mimicked this by adding noise to the labels in the data and/or by randomly removing atoms to simulate defects. This would better help our model work in a real setting.</p>
</section>
</section>
<section id="current-research-directions">
<h2><span class="section-number">8.13. </span>Current Research Directions<a class="headerlink" href="#current-research-directions" title="Permalink to this headline">¶</a></h2>
<section id="common-architecture-motifs-and-comparisons">
<h3><span class="section-number">8.13.1. </span>Common Architecture Motifs and Comparisons<a class="headerlink" href="#common-architecture-motifs-and-comparisons" title="Permalink to this headline">¶</a></h3>
<p>We’ve now seen message passing layer GNNs, GCNs, GGNs, and the generalized Battaglia equations. You’ll find common motifs in the architectures, like gating, <a class="reference internal" href="attention.html"><span class="doc">アテンションレイヤー</span></a>, and pooling strategies. For example, Gated GNNS (GGNs) can be combined with attention pooling to create Gated Attention GNNs (GAANs)<span id="id28">[<a class="reference internal" href="#id84" title="Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.">ZSX+18</a>]</span>. GraphSAGE is a similar to a GCN but it samples when pooling, making the neighbor-updates of fixed dimension<span id="id29">[<a class="reference internal" href="#id95" title="Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in neural information processing systems, 1024–1034. 2017.">HYL17</a>]</span>. So you’ll see the suffix “sage” when you sample over neighbors while pooling. These can all be represented in the Battaglia equations, but you should be aware of these names.</p>
<p>The enormous variety of architectures has led to work on identifying the “best” or most general GNN architecture <span id="id30">[<a class="reference internal" href="#id98" title="Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.">DJL+20</a>, <a class="reference internal" href="#id97" title="Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In International Conference on Learning Representations. 2019.">EPBM19</a>, <a class="reference internal" href="#id96" title="Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.">SMBGunnemann18</a>]</span>. Unfortunately, the question of which GNN architecture is best is as difficult as “what benchmark problems are best?” Thus there are no agreed-upon conclusions on the best architecture. However, those papers are great resources on training, hyperparameters, and reasonable starting guesses and I highly recommend reading them before designing your own GNN. There has been some theoretical work to show that simple architectures, like GCNs, cannot distinguish between certain simple graphs <span id="id31">[<a class="reference internal" href="#id94" title="Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations. 2018.">XHLJ18</a>]</span>. How much this practically matters depends on your data. Ultimately, there is so much variety in hyperparameters, data equivariances, and training decisions that you should think carefully about how much the GNN architecture matters before exploring it with too much depth.</p>
</section>
<section id="nodes-edges-and-features">
<h3><span class="section-number">8.13.2. </span>Nodes, Edges, and Features<a class="headerlink" href="#nodes-edges-and-features" title="Permalink to this headline">¶</a></h3>
<p>You’ll find that most GNNs use the node-update equation in the Battaglia equations but do not update edges. For example, the GCN will update nodes at each layer but the edges are constant. Some recent work has shown that updating edges can be important for learning when the edges have geometric information, like if the input graph is a molecule and the edges are distance between the atoms <span id="id32">[<a class="reference internal" href="#id110" title="Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations. 2020.">KGrossGunnemann20</a>]</span>. As we’ll see in the chapter on equivariances (<a class="reference internal" href="data.html"><span class="doc">Input Data &amp; Equivariances</span></a>), one of the key properties of neural networks with point clouds (i.e., Cartesian xyz coordinates) is to have rotation equivariance. <span id="id33">[<a class="reference internal" href="#id110" title="Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations. 2020.">KGrossGunnemann20</a>]</span> showed that you can achieve this if you do edge updates and encode the edge vectors using a rotation equivariant basis set with spherical harmonics and Bessel functions. These kind of edge updating GNNs can be used to predict protein structure <span id="id34">[<a class="reference internal" href="#id111" title="Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.">JES+20</a>]</span>.</p>
<p>Another common variation on node features is to pack more into node features than just element identity. In many examples, you will see people inserting valence, elemental mass, electronegativity, a bit indicating if the atom is in a ring, a bit indicating if the atom is aromatic, etc. Typically these are unnecessary, since a model should be able to learn any of these features which are computed from the graph and node elements. However, we and others have empirically found that some can help, specifically indicating if an atom is in a ring <span id="id35">[<a class="reference internal" href="#id112" title="Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. Chemical Science, 11(35):9524–9531, 2020.">LWC+20</a>]</span>. Choosing extra features to include though should be at the bottom of your list of things to explore when designing and using GNNs.</p>
</section>
<section id="beyond-message-passing">
<h3><span class="section-number">8.13.3. </span>Beyond Message Passing<a class="headerlink" href="#beyond-message-passing" title="Permalink to this headline">¶</a></h3>
<p>One of the common themes of GNN research is moving “beyond message passing,” where message passing is the message construction, aggregation, and node update with messages. Some view this as impossible – claiming that all GNNs can be recast as message passing <span id="id36">[<a class="reference internal" href="#id229" title="Petar Veličković. Message passing all the way up. arXiv preprint arXiv:2202.11097, 2022.">Velivckovic22</a>]</span>. Another direction is on disconnecting the underlying graph being input to the GNN and the graph used to compute updates. We sort of saw this above with SchNet, where we restricted the maximum degree for the message passing. More useful are ideas like “lifting” the graphs into more structured objects like simplicial complexes <span id="id37">[<a class="reference internal" href="#id231" title="Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: cw networks. Advances in Neural Information Processing Systems, 34:2625–2640, 2021.">BFO+21</a>]</span>. Finally, you can also choose where to send the messages beyond just neighbors <span id="id38">[<a class="reference internal" href="#id230" title="Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: automorphism-based graph neural nets. Advances in Neural Information Processing Systems, 2021.">TZK21</a>]</span>. For example, all nodes on a path could communicate messages or all nodes in a clique.</p>
</section>
<section id="do-we-need-graphs">
<h3><span class="section-number">8.13.4. </span>Do we need graphs?<a class="headerlink" href="#do-we-need-graphs" title="Permalink to this headline">¶</a></h3>
<p>It is possible to convert a graph into a string if you’re working with an adjacency matrix without continuous values. Molecules specifically can be converted into a string. This means you can use layers for sequences/strings (e.g., recurrent neural networks or 1D convolutions) and avoid the complexities of a graph neural network. SMILES is one way to convert molecular graphs into strings. With SMILES, you cannot predict a per-atom quantity and thus a graph neural network is required for atom/bond labels. However, the choice is less clear for per-molecule properties like toxicity or solubility. There is no consensus about if a graph or string/SMILES representation is better. SMILES can exceed certain graph neural networks in accuracy on some tasks. SMILES is typically better on generative tasks. Graphs obviously beat SMILES in label representations, because they have granularity of bonds/edges.  We’ll see how to model SMILES in <a class="reference internal" href="NLP.html"><span class="doc">Deep Learning on Sequences</span></a>, but it is an open question of which is better.</p>
</section>
<section id="stereochemistry-chiral-molecules">
<h3><span class="section-number">8.13.5. </span>Stereochemistry/Chiral Molecules<a class="headerlink" href="#stereochemistry-chiral-molecules" title="Permalink to this headline">¶</a></h3>
<p>Stereochemistry is fundamentally a 3D property of molecules and thus not present in the covalent bonding. It is measured experimentally by seeing if molecules rotate polarized light and a molecule is called chiral or “optically active” if it is experimentally known to have this property. Stereochemistry is the categorization of how molecules can preferentially rotate polarized light through asymmetries with respect to their mirror images. In organic chemistry, the majority of stereochemistry is of enantiomers. Enantiomers are “handedness” around specific atoms called chiral centers which have 4 or more different bonded atoms. These may be treated in a graph by indicating which nodes are chiral centers (nodes) and what their state or mixture of states (racemic) are. This can be treated as an extra processing step. Amino acids and thus all proteins are entaniomers with only one form present. This chirality of proteins means many drug molecules can be more or less potent depending on their stereochemistry.</p>
<figure class="align-default" id="helicene">
<a class="autoplay-video reference internal image-reference" href="../_images/helicene.mp4"><video class="autoplay-video" src="../_images/helicene.mp4" title="../_images/helicene.mp4" width="500"><a href="../_images/helicene.mp4">../_images/helicene.mp4</a></video></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">This is a molecule with axial stereochemistry. Its small helix could be either left or right-handed.</span><a class="headerlink" href="#helicene" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Adding node labels is not enough generally. Molecules can interconvert between stereoisomers at chiral centers through a process called tautomerization. There are also types of stereochemistry that are not at a specific atom, like rotamers that are around a bond. Then there is stereochemistry that involves multiple atoms like axial helecene. As shown in <a class="reference internal" href="#helicene"><span class="std std-numref">Fig. 8.5</span></a>, the molecule has no chiral centers but is “optically active” (experimentally measured to be chiral) because of its helix which can be left- or right-handed.</p>
</section>
</section>
<section id="relevant-videos">
<h2><span class="section-number">8.14. </span>Relevant Videos<a class="headerlink" href="#relevant-videos" title="Permalink to this headline">¶</a></h2>
<section id="intro-to-gnns">
<h3><span class="section-number">8.14.1. </span>Intro to GNNs<a class="headerlink" href="#intro-to-gnns" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/uF53xsT7mjc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
<section id="overview-of-gnn-with-molecule-compiler-examples">
<h3><span class="section-number">8.14.2. </span>Overview of GNN with Molecule, Compiler Examples<a class="headerlink" href="#overview-of-gnn-with-molecule-compiler-examples" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zCEYiCxrL_0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</section>
</section>
<section id="chapter-summary">
<h2><span class="section-number">8.15. </span>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Molecules can be represented by graphs by using one-hot encoded feature vectors that show the elemental identity of each node (atom) and an adjacency matrix that show immediate neighbors (bonded atoms).</p></li>
<li><p>Graph neural networks are a category of deep neural networks that have graphs as inputs.</p></li>
<li><p>One of the early GNNs is the Kipf &amp; Welling GCN. The input to the GCN is the node feature vector and the adjacency matrix, and returns the updated node feature vector. The GCN is permutation invariant because it averages over the neighbors.</p></li>
<li><p>A GCN can be viewed as a message-passing layer, in which we have senders and receivers. Messages are computed from neighboring nodes, which when aggregated update that node.</p></li>
<li><p>A gated graph neural network is a variant of the message passing layer, for which the nodes are updated according to a gated recurrent unit function.</p></li>
<li><p>The aggregation of messages is sometimes called pooling, for which there are multiple reduction operations.</p></li>
<li><p>GNNs output a graph. To get a per-atom or per-molecule property, use a readout function. The readout depends on if your property is intensive vs extensive</p></li>
<li><p>The Battaglia equations encompasses almost all GNNs into a set of 6 update and aggregation equations.</p></li>
<li><p>You can convert xyz coordinates into a graph and use a GNN like SchNet</p></li>
</ul>
</section>
<section id="cited-references">
<h2><span class="section-number">8.16. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">¶</a></h2>
<div class="docutils container" id="id39">
<dl class="citation">
<dt class="label" id="id98"><span class="brackets">DJL+20</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id30">2</a>)</span></dt>
<dd><p>Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. <em>arXiv preprint arXiv:2003.00982</em>, 2020.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id2">BBL+17</a></span></dt>
<dd><p>Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. <em>IEEE Signal Processing Magazine</em>, 34(4):18–42, 2017.</p>
</dd>
<dt class="label" id="id100"><span class="brackets"><a class="fn-backref" href="#id3">WPC+20</a></span></dt>
<dd><p>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2020.</p>
</dd>
<dt class="label" id="id112"><span class="brackets">LWC+20</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id35">2</a>)</span></dt>
<dd><p>Zhiheng Li, Geemi P Wellawatte, Maghesree Chakraborty, Heta A Gandhi, Chenliang Xu, and Andrew D White. Graph neural network based coarse-grained mapping prediction. <em>Chemical Science</em>, 11(35):9524–9531, 2020.</p>
</dd>
<dt class="label" id="id113"><span class="brackets"><a class="fn-backref" href="#id5">YCW20</a></span></dt>
<dd><p>Ziyue Yang, Maghesree Chakraborty, and Andrew D White. Predicting chemical shifts with graph neural networks. <em>bioRxiv</em>, 2020.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id6">XFLW+19</a></span></dt>
<dd><p>Tian Xie, Arthur France-Lanord, Yanming Wang, Yang Shao-Horn, and Jeffrey C Grossman. Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials. <em>Nature communications</em>, 10(1):1–9, 2019.</p>
</dd>
<dt class="label" id="id195"><span class="brackets"><a class="fn-backref" href="#id7">SLRPW21</a></span></dt>
<dd><p>Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alex Wiltschko. A gentle introduction to graph neural networks. <em>Distill</em>, 2021. https://distill.pub/2021/gnn-intro. <a class="reference external" href="https://doi.org/10.23915/distill.00033">doi:10.23915/distill.00033</a>.</p>
</dd>
<dt class="label" id="id136"><span class="brackets"><a class="fn-backref" href="#id9">XG18</a></span></dt>
<dd><p>Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. <em>Phys. Rev. Lett.</em>, 120:145301, Apr 2018. URL: <a class="reference external" href="https://link.aps.org/doi/10.1103/PhysRevLett.120.145301">https://link.aps.org/doi/10.1103/PhysRevLett.120.145301</a>, <a class="reference external" href="https://doi.org/10.1103/PhysRevLett.120.145301">doi:10.1103/PhysRevLett.120.145301</a>.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id12">KW16</a></span></dt>
<dd><p>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. <em>arXiv preprint arXiv:1609.02907</em>, 2016.</p>
</dd>
<dt class="label" id="id81"><span class="brackets"><a class="fn-backref" href="#id14">GSR+17</a></span></dt>
<dd><p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. <em>arXiv preprint arXiv:1704.01212</em>, 2017.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id15">LTBZ15</a></span></dt>
<dd><p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. <em>arXiv preprint arXiv:1511.05493</em>, 2015.</p>
</dd>
<dt class="label" id="id80"><span class="brackets"><a class="fn-backref" href="#id16">CGCB14</a></span></dt>
<dd><p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>arXiv preprint arXiv:1412.3555</em>, 2014.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">XHLJ18</span><span class="fn-backref">(<a href="#id17">1</a>,<a href="#id31">2</a>)</span></dt>
<dd><p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In <em>International Conference on Learning Representations</em>. 2018.</p>
</dd>
<dt class="label" id="id93"><span class="brackets"><a class="fn-backref" href="#id18">LDLio19</a></span></dt>
<dd><p>Enxhell Luzhnica, Ben Day, and Pietro Liò. On graph classification networks, datasets and baselines. <em>arXiv preprint arXiv:1905.04682</em>, 2019.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id18">MSK20</a></span></dt>
<dd><p>Diego Mesquita, Amauri Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. <em>Advances in Neural Information Processing Systems</em>, 2020.</p>
</dd>
<dt class="label" id="id199"><span class="brackets"><a class="fn-backref" href="#id19">GZBA21</a></span></dt>
<dd><p>Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding pooling in graph neural networks. <em>arXiv preprint arXiv:2110.05292</em>, 2021.</p>
</dd>
<dt class="label" id="id201"><span class="brackets"><a class="fn-backref" href="#id20">DRA21</a></span></dt>
<dd><p>Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal. Understanding convolutions on graphs. <em>Distill</em>, 2021. https://distill.pub/2021/understanding-gnns. <a class="reference external" href="https://doi.org/10.23915/distill.00032">doi:10.23915/distill.00032</a>.</p>
</dd>
<dt class="label" id="id119"><span class="brackets">ZKR+17</span><span class="fn-backref">(<a href="#id21">1</a>,<a href="#id22">2</a>)</span></dt>
<dd><p>Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In <em>Advances in neural information processing systems</em>, 3391–3401. 2017.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id23">BHB+18</a></span></dt>
<dd><p>Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. <em>arXiv preprint arXiv:1806.01261</em>, 2018.</p>
</dd>
<dt class="label" id="id227"><span class="brackets">SchuttSK+18</span><span class="fn-backref">(<a href="#id24">1</a>,<a href="#id25">2</a>,<a href="#id27">3</a>)</span></dt>
<dd><p>Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Müller. Schnet–a deep learning architecture for molecules and materials. <em>The Journal of Chemical Physics</em>, 148(24):241722, 2018.</p>
</dd>
<dt class="label" id="id228"><span class="brackets"><a class="fn-backref" href="#id26">CW22</a></span></dt>
<dd><p>Sam Cox and Andrew D White. Symmetric molecular dynamics. <em>arXiv preprint arXiv:2204.01114</em>, 2022.</p>
</dd>
<dt class="label" id="id84"><span class="brackets"><a class="fn-backref" href="#id28">ZSX+18</a></span></dt>
<dd><p>Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. <em>arXiv preprint arXiv:1803.07294</em>, 2018.</p>
</dd>
<dt class="label" id="id95"><span class="brackets"><a class="fn-backref" href="#id29">HYL17</a></span></dt>
<dd><p>Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In <em>Advances in neural information processing systems</em>, 1024–1034. 2017.</p>
</dd>
<dt class="label" id="id97"><span class="brackets"><a class="fn-backref" href="#id30">EPBM19</a></span></dt>
<dd><p>Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In <em>International Conference on Learning Representations</em>. 2019.</p>
</dd>
<dt class="label" id="id96"><span class="brackets"><a class="fn-backref" href="#id30">SMBGunnemann18</a></span></dt>
<dd><p>Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation. <em>arXiv preprint arXiv:1811.05868</em>, 2018.</p>
</dd>
<dt class="label" id="id110"><span class="brackets">KGrossGunnemann20</span><span class="fn-backref">(<a href="#id32">1</a>,<a href="#id33">2</a>)</span></dt>
<dd><p>Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional message passing for molecular graphs. In <em>International Conference on Learning Representations</em>. 2020.</p>
</dd>
<dt class="label" id="id111"><span class="brackets"><a class="fn-backref" href="#id34">JES+20</a></span></dt>
<dd><p>Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. <em>arXiv preprint arXiv:2009.01411</em>, 2020.</p>
</dd>
<dt class="label" id="id229"><span class="brackets"><a class="fn-backref" href="#id36">Velivckovic22</a></span></dt>
<dd><p>Petar Veličković. Message passing all the way up. <em>arXiv preprint arXiv:2202.11097</em>, 2022.</p>
</dd>
<dt class="label" id="id231"><span class="brackets"><a class="fn-backref" href="#id37">BFO+21</a></span></dt>
<dd><p>Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: cw networks. <em>Advances in Neural Information Processing Systems</em>, 34:2625–2640, 2021.</p>
</dd>
<dt class="label" id="id230"><span class="brackets"><a class="fn-backref" href="#id38">TZK21</a></span></dt>
<dd><p>Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: automorphism-based graph neural nets. <em>Advances in Neural Information Processing Systems</em>, 2021.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="layers.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Standard Layers</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="data.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Input Data &amp; Equivariances</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Andrew D. White<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">✕</button> <img id="wh-modal-img"> </div>
      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>