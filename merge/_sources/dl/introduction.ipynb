{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープラーニングの概要\n",
    "\n",
    "**ディープラーニング** は、 **機械学習**のカテゴリの一つです。また機械学習は **人工知能 （AI）** のカテゴリの一つです。ディープラーニングは、ニューラルネットワークを用いて回帰や分類のような機械学習を実現します。まずこの章ではディープラーニングの概要を紹介し、この後の章でさらに詳細に踏み込んでいきます。\n",
    "\n",
    "```{admonition} 読者と & 目的\n",
    "本章は {doc}`../ml/regression` および {doc}`../ml/introduction`の内容を前提とします。本章を通じて、あなたは次のことを学ぶでしょう：\n",
    "  * ディープラーニングの定義\n",
    "  * ニューラルネットワークの定義\n",
    "  * これまでに学んだ回帰の原理のニューラルネットワークへの応用\n",
    "```\n",
    "\n",
    "この本の目的は、化学および材料科学を中心としたディープラーニングの入門書となることです。ディープラーニングに関しては、本書の他に数多くの優れた資料があり、ここでそれらの一部に触れておくことにします。これらのリソースでは、特定のトピックについてのより詳細な説明や、本書で扱わないトピック（例えば、画像認識）についての説明がなされています。まずディープラーニングの入門に関しては、[Ian Goodfellowの本](https://www.deeplearningbook.org/contents/intro.html)が良い入門書だと思いました。ビジョンについて深く知りたいなら、Grant Sandersonがニューラルネットワークに特化した[short video series](https://www.youtube.com/watch?v=aircAruvnKk)を公開しており、このトピックの応用的な紹介がされています。DeepMindは、[ディープラーニングとAI](https://www.youtube.com/watch?v=7R52wiUgxZI)で何が実現できるかを示すハイレベルなビデオを公開しています。もし研究論文で「ディープラーニングは強力なツールである」と書く場合、一般的にはヤン・ルカン、ヨシュア・ベンジオ、ジェフリー・ヒントンにより執筆されNatureに掲載された [この論文] (https://www.nature.com/articles/nature14539)が引用されることが多いでしょう。Zhang、 Lipton、 Li、および Smolaは、Tensorflow、PyTorch、MXNetといった代表的なディープラーニングフレームワークで実装されたexampleを含む実用的な[オンラインブック](http://d2l.ai/index.html)を公開しています。また、[DeepChem](https://deepchem.io/)プロジェクトでは、化学におけるディープラーニングの応用について、化学にフォーカスした多くのexampleと情報が提供されています。最後に、いくつかのディープラーニングパッケージは、そのAPIのチュートリアルを介して深層学習の短い導入を提供します： [Keras](https://keras.io/getting_started/), [PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "\n",
    "私がディープラーニングの初心者に伝える主なアドバイスは、神経学に着想を得た用語や概念（すなわち、ニューロン間の接続）にはあまり囚われすぎず、その代わりにディープラーニングを、調整可能なパラメータをたくさん含む行列を使った一連の線形代数演算として捉えることです。もちろん、ディープラーニングの線形代数演算を結合するために使われる非線形関数（活性化）など、神経学と類似した概念もところどころに登場しますが、ニューラルネットワークは神経学の延長にあるものではなく、それとは切り離された別なものとして学ぶことが適切と言えます。例え脳内で接続されたニューロンのように見えたとしても―――ディープラーニングは本質的に、「計算ネットワーク」（計算グラフとも呼ばれる）によって記述される線形代数演算です。\n",
    "\n",
    "\n",
    "```{admonition} 非線形性\n",
    "関数 $f(\\vec{x})$ が次の2つの条件を満たすなら、$f(\\vec{x})$は線形である：\n",
    "\n",
    "任意の $\\vec{x}$ および $\\vec{y}$ について、\n",
    "\\begin{equation}\n",
    "f(\\vec{x} + \\vec{y}) = f(\\vec{x}) + f(\\vec{y})\n",
    "\\end{equation}\n",
    "\n",
    "また、\n",
    "\n",
    "\\begin{equation}\n",
    "f(s\\vec{x}) = sf(\\vec{x})\n",
    "\\end{equation}\n",
    "\n",
    "ここで $s$ はスカラーである。\n",
    "もし$f(\\vec{x})$がこれらの条件を満たさない場合、$f(\\vec{x})$は **非線形** である。\n",
    "```\n",
    "\n",
    "\n",
    "## ニューラルネットワークとは？\n",
    "ディープラーニングにおける *ディープ* とは、ニューラルネットワークが何層ものレイヤーから構成されることを意味します。では、ニューラルネットワークとは何でしょうか？一般化した言い方をすれば、ニューラルネットワークは2つの要素で構成されると考えることができます：（1） 入力特徴 $\\mathbf{X}$ に対して非線形変換 $g(\\cdot)$ を適用し、新しい特徴 $\\mathbf{H} = g(\\mathbf{X})$ を出力する部分、（2） 既に {doc}`../ml/introduction` で見たような線形モデル。我々のディープラーニングによる回帰モデルの式は次のようになっています。\n",
    "\n",
    "\\begin{equation}\n",
    "   \\hat{y} = \\vec{w}g(\\vec{x}) + b\n",
    "\\end{equation}\n",
    "\n",
    "MLの章では、特徴量の選択がいかに難解かつ困難かという点が主に議論されてきました。ここでは、これまで人手で設計されてきた特徴量を、学習可能な特徴の集合 $g(\\vec{x})$ に置き換え、これまでと同じ線形モデルを使います。それでは $g(\\vec{x})$ はどのように設計すればよいのかと気になるでしょうが、これがまさにディープラーニングの部分です。 $g(\\vec{x})$ は **レイヤー （層）** によって構成される微分可能な関数で、層それ自身も微分可能かつ、学習可能な重み（自由変数）を持ちます。ディープラーニングは成熟した分野であり、目的ごとに標準的な層が確立されています。例えば、畳み込み層は、入力テンソルの各要素の周辺について、固定された広さで近傍を見るために使われ、ドロップアウト層は、正則化の一種として入力ノードをランダムに不活性化するために使われます。最もよく使われる基本的な層は、**全結合層 （fully-connected layer）** あるいは **密結合層（dense layer）** と呼ばれるものです（訳注：以下の説明で、原文ではdense layerの呼称が使われていますが、日本語では全結合層としています）。\n",
    "\n",
    "```{margin}\n",
    "全結合層とは、各々の入力要素が出力要素全てに影響を与えることを意味する。一時期、スパースな層は、脳がどのように接続されているかという良いアナロジーと共に盛んに研究されていました。しかし、スパース層では入出力をどう結合するか決める必要がある一方、全結合層は、（スパース層と異なり全ての入出力ノードが結合したものであるため）その必要がなく、結果としてスパース層は現在ほぼ使われなくなりました。ただ例外として、畳み込みのように、他の目的のため結果的にスパースとなる層は今でも使われる場合があります。\n",
    "```\n",
    "\n",
    "全結合層は、所望の出力特徴のshapeと**活性化**の2つで定義されます。全結合層の式は次のようになります：\n",
    "\n",
    "\\begin{equation}\n",
    "     \\vec{h} = \\sigma(\\mathbf{W}\\vec{x} + \\vec{b})\n",
    "\\end{equation}\n",
    "\n",
    "ここで、$\\mathbf{W}$ は学習可能な $D \\times F$ 行列、$D$ は入力ベクトル ($vec{x}$) の次元、$F$ は出力ベクトル ($vec{h}$) の次元、$vec{b}$ は学習可能な $F$ 次元ベクトル、$\\sigma(\\cdot)$ は活性化関数です。出力特徴数の $F$ や $\\sigma(\\cdot)$ はこのモデルの**ハイパーパラメータ**の一つで、つまりモデルの学習時に自動的に獲得される値ではなく、問題ごとにチューニングすべき値です。 活性化関数には、微分可能かつ値域が $(-\\infty, \\infty)$ であれば、基本的にどのような関数も使用できます。ただし、活性化関数が線形関数の場合、複数の全結合層を重ねたとしても単に行列を定数倍したことと等価であり、結局のところ線形回帰になってしまいます。よって、ニューラルネットワークが非線形性をもつ関数を表現できるよう、活性化関数には非線形な関数を用いるのが普通です。また非線形性だけでなく、活性化関数はオン・オフが可能、つまり、入力値のある領域に対して出力値が0になる性質が必要です。一般に、負の入力に対して活性化関数はゼロもしくはそれに近い値になります。\n",
    "\n",
    "これら2つの性質を備えた最もシンプルな活性化関数は、正規化線形関数（rectified linear unit, ReLU)で、以下のように表されます：\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\left\\{\\begin{array}{lr}\n",
    "x & x > 0\\\\\n",
    "0 & \\textrm{otherwise}\\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "### 万能近似定理\n",
    "\n",
    "ニューラルネットワークが未知関数 ($f(\\vec{x})$) の近似に適している理由の一つは、ネットワークの深さ（層の数）や幅（隠れ層の大きさ）が十分に大きければ、どんな関数でも近似できることです（万能近似定理）。この定理には多くのバリエーションがあり、無限に広いニューラルネットや無限に深いニューラルネットについての証明が示されています。例えば、任意の1次元の関数は、無限に広い層（無限の隠れ層次元）を持ち活性化関数としてReLUが用いられた、深さ5のニューラルネットワークで近似できることが知られています{cite}`lu2017expressive`。\n",
    "\n",
    "### Frameworks\n",
    "\n",
    "ディープラーニングの実装には、多くの「ハマりポイント」（※）、つまり容易にミスを犯してしまうポイントがたくさんあるため、ニューラルネットワークや学習に必要な様々な機能を自分でゼロから実装することは難しいです。特に数値的な不安定性は、モデルが学習に失敗した時にはじめて気づく場合が多く、厄介な問題になります。\n",
    "そこでこの本のいくつかの例では、JAXの代わりに少し抽象的なソフトウェアフレームワークを用いることにします。ここでは、人気のあるディープラーニングフレームワークの1つである[Keras](https://keras.io/)を用います。Kerasは高度な処理を非常にシンプルなコードで記述できることから、ディープラーニングの実例を簡潔に示すために適していると言えます。\n",
    "\n",
    "```{margin}\n",
    "※ 訳注：原文では gotcha でした。gotchaは I got you. = I understand you. といった意味で使われることが多いスラングですが、[「みんなが踏む落とし穴」といった意味もある](https://www.urbandictionary.com/define.php?term=gotcha&defid=8760478)そうです。訳者（鈴木）の友人曰く、こういった落とし穴を踏んだ人に対して「お前踏んだの、わかる～」的ニュアンスで I got you. と言われることに由来するようです。\n",
    "```\n",
    "\n",
    "### ディスカッション\n",
    "この本では、ディープラーニングそのものの紹介はなるべくあっさりと済ませます。世の中にはディープラーニングに関する優れた学習教材があります。上記の読み物や、Keras（またはPyTorch）のチュートリアルを使って、ニューラルネットワークや学習の概念に慣れておくとよいでしょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 溶解度予測モデルを振り返る\n",
    "\n",
    "ディープラーニングの最初の例として、2層の全結合ニューラルネットワークを使って溶解度データセットを再度学習してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このノートブックの動かし方\n",
    "\n",
    "このページ上部の &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; を押すと、このノートブックがGoogle Colab.で開かれます。必要なパッケージのインストール方法については以下を参照してください。\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "必要なパッケージをインストールするには、新規セルを作成して次のコードを実行してください。\n",
    "\n",
    "```\n",
    "!pip install dmol-book\n",
    "```\n",
    "もしインストールがうまくいかない場合、パッケージのバージョン不一致が原因である可能性があります。動作確認がとれた最新バージョンの一覧は[ここ](https://github.com/whitead/dmol-book/blob/master/package/requirements.txt)から参照できます\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import dmol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み\n",
    "\n",
    "データをダウンロードし、[Pandas](https://pandas.pydata.org/)のdata frameとして読み込みます。さらに、以前と同じようにして特徴量を正規化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\n",
    "# had to rehost because dataverse isn't reliable\n",
    "soldata = pd.read_csv(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv\"\n",
    ")\n",
    "features_start_at = list(soldata.columns).index(\"MolWt\")\n",
    "feature_names = soldata.columns[features_start_at:]\n",
    "# standardize the features\n",
    "soldata[feature_names] -= soldata[feature_names].mean()\n",
    "soldata[feature_names] /= soldata[feature_names].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras用にデータを準備\n",
    "\n",
    "ディープラーニングライブラリを使うことで、データの分割や各レイヤーの構築など、多くの一般的タスクが簡単に行えます。以下のコードでは、numpyのarrayからKerasのデータセットを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (soldata[feature_names].values, soldata[\"Solubility\"].values)\n",
    ")\n",
    "N = len(soldata)\n",
    "test_N = int(0.1 * N)\n",
    "test_data = full_data.take(test_N).batch(16)\n",
    "train_data = full_data.skip(test_N).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコード内における `skip` や `take` (次を参照 {obj}`tf.data.Dataset`) は、データセットを2つに分割し、ミニバッチを作る操作です。\n",
    "\n",
    "## ニューラルネットワーク\n",
    "では、ニューラルネットワークモデルを構築しましょう。この場合、 $g(\\vec{x}) = \\sigma\\left(\\mathbf{W^0}\\vec{x} + \\vec{b}\\right)$ となります。この関数 $g(\\vec{x})$ を *隠れ層（hidden layer）* と呼ぶことにします。これは、我々は $g(\\vec{x})$ の出力をそのまま最終的な結果として扱うわけではないからです。予測したい溶解度は $y = \\vec{w}g(\\vec{x}) + b$ となることに注意してください。活性化関数 $sigma( \\cdot)$ はtanh、隠れ層の出力次元は32とします。非線形な活性化関数はたくさんありますが、ここでtanhを選んだのは経験的な理由です。このように、活性化関数は一般的に効率と経験的な性能に基づいて選択されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our hidden layer\n",
    "# We only need to define the output dimension - 32.\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"tanh\")\n",
    "# Last layer - which we want to output one number\n",
    "# the predicted solubility.\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "# Now we put the layers into a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hidden_layer)\n",
    "model.add(output_layer)\n",
    "\n",
    "# our model is complete\n",
    "\n",
    "# Try out our model on first few datapoints\n",
    "model(soldata[feature_names].values[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} Jax vs Keras\n",
    "この例をJaxで実装することも可能ですが、その場合はコードが数行増えることになります。\n",
    "この章では、ローレベルな実装よりもディープラーニングの実装の高水準な点に着目するためにKerasを用いました。\n",
    "```\n",
    "\n",
    "上の3つの分子に対する溶解度を予測するモデルができました。Pandasのデータはデータ精度にfloat64（倍精度浮動小数点）を使っているのに、我々のモデルはfloat32（単精度）を使っているという警告が出るかもしれませんが、これはそれほど重要ではありません。この警告は技術的理由から数値の精度を少し落としているためですが、分子の溶解度は32ビットと64ビット精度の浮動小数点による誤差よりもはるかに多くの分散を持っていることから、この誤差の影響は無視できます。最後の行を次のように修正すれば、この警告を消すことができます。\n",
    "\n",
    "\n",
    "```py\n",
    "model(soldata[feature_names].values[:3].astype(float))\n",
    "```\n",
    "\n",
    "ここまでで、我々はディープニューラルネットワークのモデル構造を目的に従って定義し、データに対して呼び出すことができるようになりました。あとはこのモデルを学習させるだけです。{obj}`model.compile<tf.keras.Model>` を呼び出し、最適化器（通常は確率的勾配降下法の一種）と損失関数を定義することで、学習用モデルの準備は完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerasを使うと簡単にディープラーニングモデルが定義できることに気づきましたか？ 以前に損失と最適化器を準備するためにかかった労力を振り返ってみてください。これがディープラーニングフレームワークを使う利点です。では、これでモデルを学習するための準備が整いました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習も簡単ですね！\n",
    "\n",
    "```{margin}\n",
    "エポックとは、バッチサイズに関係なく、データセット全体に対して行う繰り返し操作の単位である。\n",
    "```\n",
    "\n",
    "参考までに、前回のベースラインモデルではlossの出力は3くらいでした。また、Kerasによる最適化により、処理はより高速になりました。では、このモデルのテストデータにおける性能を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# get model predictions on test data and get labels\n",
    "# squeeze to remove extra dimensions\n",
    "yhat = np.squeeze(model.predict(test_data))\n",
    "test_y = soldata[\"Solubility\"].values[:test_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_y, yhat, \".\")\n",
    "plt.plot(test_y, test_y, \"-\")\n",
    "plt.xlabel(\"Measured Solubility $y$\")\n",
    "plt.ylabel(\"Predicted Solubility $\\hat{y}$\")\n",
    "plt.text(\n",
    "    min(test_y) + 1,\n",
    "    max(test_y) - 2,\n",
    "    f\"correlation = {np.corrcoef(test_y, yhat)[0,1]:.3f}\",\n",
    ")\n",
    "plt.text(\n",
    "    min(test_y) + 1,\n",
    "    max(test_y) - 3,\n",
    "    f\"loss = {np.sqrt(np.mean((test_y - yhat)**2)):.3f}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性能も、ベースラインモデルよりかなり良いことがわかりｍす。\n",
    "This performance is better than our simple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習問題\n",
    "\n",
    "1. ReLU関数をグラフにプロットし、ReLUが非線形関数であることを確認せよ \n",
    "2. バイアス・バリアンストレードオフのことは一旦忘れて、ニューラルネットワークの層の数を増やすことにチャレンジしてみてください\n",
    "3. $\\sigma(\\cdot)$ が恒等関数であれば、ニューラルネットワークは線形回帰であることを示せ。\n",
    "4. データのフィッティングに、非線形回帰ではなく、ディープラーニングを使うメリットとデメリットは何ですか？また、どのような場合にディープラーニングではなく非線形回帰を選択しますか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この章の目次\n",
    "\n",
    "* ディープラーニングとは、機械学習の一種で、ニューラルネットワークを利用してデータの分類や回帰を行うものです。\n",
    "* ニューラルネットワークは、調整可能なパラメータを持つ行列を用いた一連の演算です。\n",
    "* ニューラルネットワークは、入力された特徴を、その後回帰や分類に使用できる新しい特徴の集合に変換します。\n",
    "* 最も一般的な層は全結合層で、全結合層において各入力要素は各出力要素に影響を与えます。全結合層のパラメータは望ましい出力特徴の形と活性化関数によって定義されます。\n",
    "* 十分な数の層、または十分な幅の隠れ層があれば、ニューラルネットワークは未知の関数を近似することができます。\n",
    "* 隠れ層は、その出力を我々は普段観測しないため、このように呼ばれています。\n",
    "* TensorFlowなどのライブラリを使うと、データをトレーニングとテストに分けるだけでなく、ニューラルネットワークの層を構築することも容易になります。\n",
    "* ニューラルネットワークを構築することで、分子の溶解度など様々な物性を予測することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
