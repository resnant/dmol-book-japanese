{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習入門\n",
    "\n",
    "*機械学習入門*と呼ばれる記事は、おそらくwebサイト上に数千件あります。本稿では、これらの記事を書き直すのではなく、化学での応用に焦点を当てた主要な概念を紹介することにします。 以下に入門的な資料を挙げます。新しいより良いものがほかにあれば教えてください。\n",
    "\n",
    "1. 原著者が大学院で初めて読んだ、機械学習についてのEthem Alpaydinによって書かれた本{cite}`alpaydin2020introduction`\n",
    "2. Nils Nillsonのオンラインブック [<ins>Introductory Machine Learning</ins>](https://ai.stanford.edu/~nilsson/mlbook.html)\n",
    "3. 物質科学における機械学習についての2つのreview{cite}`fung2021benchmarking,balachandran2019machine`\n",
    "4. 計算科学における機械学習についての2つのreview{cite}`gomez2020machine`\n",
    "5. 金属科学における機械学習についての2つのreview{cite}`nandy2018strategies`\n",
    "\n",
    "これらの資料から、機械学習がデータをモデリングする手法であり、一般的にはには予測機能を持つことを学んでいただけると思います。機械学習には多くの手法が含まれますが、ここでは深層学習を学ぶために必要なものだけを取り上げます。例えば、ランダムフォレスト、サポートベクターマシン、最近傍探索などは広く使われている機械学習手法で、今でも有効な手法ですが、ここでは取り上げません。\n",
    "\n",
    "```{admonition} 読者層と目的\n",
    "本章は、化学とpythonについてある程度知識のある機械学習の初心者を対象としており、そうでない場合には上記の入門記事のいずれかに目を通しておくことをお勧めします。この記事では、`pandas`の知識（カラムの読み込みと選択）、`rdkit`の知識（分子の描き方）、分子を[SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) {cite}`weininger1988smiles`として保存する方法についてある程度の知識を想定しています。この章を読むと、以下のことができるようになると想定されます。\n",
    "\n",
    "  * 特徴量、ラベルの定義\n",
    "  * 教師あり学習と教師なし学習を区別できる。\n",
    "  * 損失関数とは何か、勾配降下法を用いてどのように最小化できるかを理解する。\n",
    "  * モデルとは何か、特徴とラベルとの関係を理解する。\n",
    "  * データのクラスタリングができ、それがデータについて何を示すかを説明できる。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用語の説明\n",
    "\n",
    "機械学習とは、データに当てはめてモデルを構築することを目指す分野です。\n",
    "まず、言葉を定義します。\n",
    "\n",
    "**特徴量** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;次元$D$の$N$個のベクトル $\\{\\vec{x}_i\\}$ の集合です。実数、整数等が用いられます。\n",
    "\n",
    "**ラベル** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N$ 個の整数または実数の集合 $\\{y_i\\}$。$y_i$ は通常スカラーです。\n",
    "  \n",
    "**ラベル付きデータ** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N$ 個のtupleからなる集合 $\\{\\left(\\vec{x}_i, y_i\\right)\\}$ を指します。\n",
    "\n",
    "**ラベルなしデータ** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ラベル $y$ が未知の $N$ 個の特徴量 $\\{\\vec{x}_i\\}$ の集合を指します。\n",
    "\n",
    "**モデル**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;特徴ベクトルを受け取り、予測結果 $\\hat{y}$ を出力する関数 $f(\\vec{x})$ を指します。\n",
    "\n",
    "**予測結果**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 与えられた入力 $\\vec{x}$ に対し、モデルを通して得られた予測結果 $\\hat{y}$ のことを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師あり学習\n",
    "\n",
    "最初のタスクは**教師あり学習**です。教師あり学習とは、データで学習したモデルで $\\vec{x}$ から $y$ を予測する方法です。このタスクは、データセットに含まれるラベルをアルゴリズムに教えることで学習を進めるため、*教師あり*学習と呼ばれています。もう一つの方法は**教師なし学習**で、アルゴリズムにラベルを教えない方法です。この教師あり／教師なしの区別は後でもっと厳密になりますが、今のところはこの定義で十分です。\n",
    "\n",
    "例として、AqSolDB{cite}`Sorkun2019`という、約1万種類の化合物と、その水への溶解度の測定結果(ラベル)についてのデータセットを使ってみます。このデータセットには、機械学習に利用できる分子特性（特徴量）も含まれています。溶解度の測定結果は、化合物の水への溶解度をlog molarityの単位で表したものになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebookの実行\n",
    "\n",
    "\n",
    "上にある &nbsp;<i aria-label=\"Launch interactive content\" class=\"fas fa-rocket\"></i>&nbsp; をクリックすると、このページがインタラクティブなGoogle Colab Notebookとして起動されるようになります。 パッケージのインストールについては、以下を参照してください。\n",
    "\n",
    "````{tip} My title\n",
    ":class: dropdown\n",
    "パッケージをインストールするには、新しいセルで次のコードを実行します。\n",
    "\n",
    "```\n",
    "!pip install dmol-book\n",
    "```\n",
    "\n",
    "インストールに問題が生じた場合は、[このリンク](https://github.com/whitead/dmol-book/blob/master/package/requirements.txt)から使用されているパッケージリストの最新版を入手することができます。\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロード\n",
    "\n",
    "データをダウンロードし、[Pandas](https://pandas.pydata.org/)のデータフレームにロードします。以下のセルでは、インポートや必要なパッケージのインストールを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.example_libraries import optimizers\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.Draw\n",
    "import dmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\n",
    "# had to rehost because dataverse isn't reliable\n",
    "soldata = pd.read_csv(\n",
    "    \"https://github.com/whitead/dmol-book/raw/master/data/curated-solubility-dataset.csv\"\n",
    ")\n",
    "soldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ探索\n",
    "\n",
    "```{margin} EDA\n",
    "EDAを特徴量の選択として行う場合、テストデータでモデルの選択に影響を与えないように、EDAを行う前にtrain/test/(valid)の分割を行う必要があります。\n",
    "```\n",
    "\n",
    "分子には、分子量、回転可能な結合、価電子など、様々な特徴量となりうるものがあります。そしてもちろん、今回のデータセットにおいてラベルとなる**溶解度**という指標もあります。このデータセットに対して私たちが常に最初に行うべきことの一つは、**探索的データ解析**（EDA）と呼ばれるプロセスでデータについての理解を深めることです。まず、ラベルやデータの大枠を知るために、いくつかの具体的な例を調べることから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one molecule\n",
    "mol = rdkit.Chem.MolFromInchi(soldata.InChI[0])\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはデータセットのうち、最初の分子を[rdkit](https://rdkit.org/)を使ってレンダリングしたものです。\n",
    "\n",
    "それでは、溶解度データの**範囲**とそれを構成する分子についてなんとなく理解するために、極値を見てみましょう。まず、溶解度の確率分布の形と極値を知るために、({obj}`seaborn.distplot` を使って)溶解度のヒストグラムを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(soldata.Solubility)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図では、溶解度のヒストグラムとカーネル密度推定値を重ね合わせています。このヒストグラムから、溶解度は約-13から2.5まで変化し、正規分布していないことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 3 lowest and 3 highest solubilities\n",
    "soldata_sorted = soldata.sort_values(\"Solubility\")\n",
    "extremes = pd.concat([soldata_sorted[:3], soldata_sorted[-3:]])\n",
    "\n",
    "# We need to have a list of strings for legends\n",
    "legend_text = [\n",
    "    f\"{x.ID}: solubility = {x.Solubility:.2f}\" for x in extremes.itertuples()\n",
    "]\n",
    "\n",
    "# now plot them on a grid\n",
    "extreme_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in extremes.InChI]\n",
    "rdkit.Chem.Draw.MolsToGridImage(\n",
    "    extreme_mols, molsPerRow=3, subImgSize=(250, 250), legends=legend_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "極端な分子の例では、高塩素化合物が最も溶解度が低く、イオン性化合物が溶解度が高いことがわかります。A-2918は**外れ値**、つまり間違いなのでしょうか？また、NH$_3$ は本当にこれらの有機化合物に匹敵するのでしょうか？このような疑問は、モデリングを行う*前に*検討すべきことです。\n",
    "\n",
    "```{margin} 外れ値\n",
    "\n",
    "外れ値とは、正規のデータ分布から外れた極端な値のことです。間違いであったり、異なる分布であったりします。（例えば、有機分子ではなく金属であるなど）外れ値はモデル学習に強い影響を与える可能性があります。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の相関\n",
    "次に、特徴量と溶解度(ラベル)の相関を調べてみましょう。 `SD` (標準偏差)、`Ocurrences` (その分子が構成するデータベースで何回出現したか)、`Group` (データの出所) など、特徴量や溶解度とは関係のないカラムがいくつかあることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_start_at = list(soldata.columns).index(\"MolWt\")\n",
    "feature_names = soldata.columns[features_start_at:]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=5, ncols=4, sharey=True, figsize=(12, 8), dpi=300)\n",
    "axs = axs.flatten()  # so we don't have to slice by row and column\n",
    "for i, n in enumerate(feature_names):\n",
    "    ax = axs[i]\n",
    "    ax.scatter(\n",
    "        soldata[n], soldata.Solubility, s=6, alpha=0.4, color=f\"C{i}\"\n",
    "    )  # add some color\n",
    "    if i % 4 == 0:\n",
    "        ax.set_ylabel(\"Solubility\")\n",
    "    ax.set_xlabel(n)\n",
    "# hide empty subplots\n",
    "for i in range(len(feature_names), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分子量や水素結合の数は、少なくともこのプロットからは、ほとんど相関がないように見えるのは興味深いことです。MolLogPは溶解性に関連する計算から導出された記述子で、よい相関を持っています。また、これらの特徴量のいくつかは、**分散**が低く、特徴の値が多くのデータに対してほとんど変化しないか、全く変化しないことがわかります（例えば、「NumHDonors」など）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形モデル\n",
    "\n",
    "まず、最も単純なアプローチの1つである線形モデルから始めましょう。これは教師あり学習の最初の例で、これから説明する特徴量の選択が難しいため、ほとんど使われることはありません。\n",
    "\n",
    "\n",
    "```{margin} Autodiff\n",
    "[Autodiff](https://en.wikipedia.org/wiki/Automatic_differentiation) は2つの変数に関する分析的な勾配を計算することができるツールです。\n",
    "```\n",
    "\n",
    "この線形モデルは以下の方程式で定義されます。\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\vec{w} \\cdot \\vec{x} + b\n",
    "\\end{equation}\n",
    "\n",
    "この式は1つのデータ点に対して定義されます。1つの特徴ベクトル $\\vec{x}$ の形状は、(17個の特徴があるため)今回の場合17です。$\\vec{w}$ は長さ17の調整可能なパラメータのベクトルで、 $b$ は調整可能なスカラーです(**バイアス** と呼ばれます)。\n",
    "\n",
    "このモデルは、[``jax``](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)というライブラリを用いて実装します。このライブラリは、autodiffによって解析的勾配を簡単に計算できることを除けば、numpyに非常によく似ています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, w, b):\n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "\n",
    "# test it out\n",
    "x = np.array([1, 0, 2.5])\n",
    "w = np.array([0.2, -0.5, 0.4])\n",
    "b = 4.3\n",
    "\n",
    "linear_model(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} 損失\n",
    "損失とは、モデルの予測値 $\\hat{y}$ とラベル $y$ を受け取り、モデルがどれだけ適合しているかを表す関数から出力されるスカラーです。今回の目標は、この損失を最小化することです。\n",
    "```\n",
    "\n",
    "ここで重要な問題が出てきます。 *調整可能なパラメータ $\\vec{w}$ と $b$ はどのように見つけるのでしょうか？* 線形回帰の古典的な方法では、 $\\vec{w} = (X^TX)^{-1}X^{T}\\vec{y}$ という擬似逆行列を使って調整パラメータを直接計算します。詳しくは [こちら](https://nbviewer.jupyter.org/github/whitead/numerical_stats/blob/master/unit_12/lectures/lecture_1.ipynb#Extending-Least-Squares-to-Multiple-Dimensions-in-Domain---OLS-ND)に詳しく書いてあります。 しかし、今回は、深層学習で行うことを考慮した**反復的**なアプローチを使用します。これは線形回帰の正しい計算方法ではありませんが、深層学習ではよく見る方法なので、反復的な計算方法に慣れるのに便利でしょう。\n",
    "\n",
    "調整可能なパラメータを繰り返し見つけるために、**損失関数**を選び、**勾配**を用いて最小化することにします。これらの量を定義し、いくつかの初期値wとbを用いて損失を計算していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into features, labels\n",
    "features = soldata.loc[:, feature_names].values\n",
    "labels = soldata.Solubility.values\n",
    "\n",
    "feature_dim = features.shape[1]\n",
    "\n",
    "# initialize our paramaters\n",
    "w = np.random.normal(size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "# define loss\n",
    "def loss(y, labels):\n",
    "    return jnp.mean((y - labels) ** 2)\n",
    "\n",
    "\n",
    "# test it out\n",
    "y = linear_model(features, w, b)\n",
    "loss(y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "溶解度が-13から2であることを考えると、この損失はひどいものです。しかし、この結果はあくまで初期パラメータから予測しただけなので、この場合においては正しいのです。\n",
    "\n",
    "\n",
    "### 勾配降下法\n",
    "\n",
    "ここで、調整可能なパラメータに対して損失がどのように変化するかという情報を使って、損失を減らしていきます。\n",
    "今回用いる損失関数を以下のように定義します。:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_i^N \\left[y_i - f(\\vec{x}_i, \\vec{w}, b)\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "この損失関数は**平均2乗誤差**と呼ばれ、しばしばMSEと略されます。調整可能なパラメータに対する損失の勾配を計算することができます。\n",
    "\n",
    "```{margin} jax.grad\n",
    "[jax.grad](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)はPython関数の解析的導関数を計算します。\n",
    "これは2つの引数を取ります。関数と、どの引数に対して微分を行うかについてです。\n",
    "例えば、`f(x, y, z)`について考えると、`jax.grad(f,(1,2))` は $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}$ を与えます。 $x$ はテンソルでも良いことに注意してください。 \n",
    "```\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial w_i}, \\frac{\\partial L}{\\partial b}\n",
    "\\end{equation}\n",
    "\n",
    "ここで、$w_i$ は重みベクトル $i$ 番目の要素である $\\vec{w}$ です。負の勾配の方向に微小量の変化を生むことで、損失を減らすことができます。:\n",
    "\n",
    "\\begin{equation}\n",
    "    (w_i, b') = \\left(w_i - \\eta \\frac{\\partial L}{\\partial w_i}, b - \\eta\\frac{\\partial L}{\\partial b}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "ここで、 $\\eta$ は**学習率**であり、調整可能であるが、学習しないパラメータ（**ハイパーパラメータ**と呼ばれます）です。この例では $1\\times10^{-6}$ と設定しています。一般的には10の累乗で表され、最大でも0.1程度になるように設定されます。それ以上の値は安定性に問題があることが知られています。それでは、**gradient descent**と呼ばれるこの手順を実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients\n",
    "def loss_wrapper(w, b, data):\n",
    "    features = data[0]\n",
    "    labels = data[1]\n",
    "    y = linear_model(features, w, b)\n",
    "    return loss(y, labels)\n",
    "\n",
    "\n",
    "loss_grad = jax.grad(loss_wrapper, (0, 1))\n",
    "\n",
    "# test it out\n",
    "loss_grad(w, b, (features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配の計算を行ったので、それを数ステップかけて最小化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_progress = []\n",
    "eta = 1e-6\n",
    "data = (features, labels)\n",
    "for i in range(10):\n",
    "    grad = loss_grad(w, b, data)\n",
    "    w -= eta * grad[0]\n",
    "    b -= eta * grad[1]\n",
    "    loss_progress.append(loss_wrapper(w, b, data))\n",
    "plt.plot(loss_progress)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Full Dataset Training Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習曲線\n",
    "\n",
    "上の図は **学習曲線** と呼ばれるものです。これは損失が減少しているかどうかを示しており、モデルが学習を行っていることを表しています。学習曲線は **Learning Curve** とも呼ばれます。X軸はサンプル数、データセットの総反復回数（エポックと呼ばれる）、モデルの学習に使用されたデータ量の他の指標が用いられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ処理\n",
    "\n",
    "```{margin} バッチ\n",
    "バッチはデータのサイズに応じたデータの部分集合のことです。バッチサイズは通常2の累乗で定義されます。（例：4、16、128）。\n",
    "ランダムなバッチを用いることで、勾配降下法は確率的勾配降下法になります。\n",
    "```\n",
    "\n",
    "勾配降下法によって学習が良い感じに進んでいることがわかります。しかし、ちょっとした変更で学習のスピードアップを図ることができます。機械学習で実際に行われている学習方法である **バッチ処理** を使ってみましょう。ちょっとした変更点とは、すべてのデータを一度に使うのではなく、その部分集合の小さな**バッチ**データだけを取るということです。バッチ処理には2つの利点があります。1つはパラメータの更新を計算する時間を短縮できること、もう1つは学習過程をランダムにできることです。このランダム性により、学習の進行を止める可能性のあるる局所的な極小値から逃れることができます。このバッチ処理の追加により、この勾配降下法アルゴリズムは**確率的**となり、**確率的勾配降下法**（SGD）と呼ばれる手法になります。SGDとそのバリエーションは、深層学習における最も一般的な学習方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our paramaters\n",
    "# to be fair to previous method\n",
    "w = np.random.normal(size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "loss_progress = []\n",
    "eta = 1e-6\n",
    "batch_size = 32\n",
    "N = len(labels)  # number of data points\n",
    "data = (features, labels)\n",
    "# compute how much data fits nicely into a batch\n",
    "# and drop extra data\n",
    "new_N = len(labels) // batch_size * batch_size\n",
    "\n",
    "# the -1 means that numpy will compute\n",
    "# what that dimension should be\n",
    "batched_features = features[:new_N].reshape((-1, batch_size, feature_dim))\n",
    "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
    "# to make it random, we'll iterate over the batches randomly\n",
    "indices = np.arange(new_N // batch_size)\n",
    "np.random.shuffle(indices)\n",
    "for i in indices:\n",
    "    # choose a random set of\n",
    "    # indices to slice our data\n",
    "    grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
    "    w -= eta * grad[0]\n",
    "    b -= eta * grad[1]\n",
    "    # we still compute loss on whole dataset, but not every step\n",
    "    if i % 10 == 0:\n",
    "        loss_progress.append(loss_wrapper(w, b, data))\n",
    "\n",
    "plt.plot(np.arange(len(loss_progress)) * 10, loss_progress)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Batched Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで注目すべき点は、以下の3つです。\n",
    "\n",
    "1. バッチ処理を行わない場合に比べ、損失が小さくなっています。\n",
    "2. データセットを10回で反復するのをやめ、1回だけ反復しているにも関わらず、ステップ数が増えています。\n",
    "3. 損失は常に減少するわけではありません。\n",
    "\n",
    "損失が小さくなる理由は、各データポイントを1回しか見ていないにもかかわらず、より多くのステップを踏むことができるからです。バッチ処理を行うと、バッチごとに勾配降下法の更新を行うため、データセットに対して1回の反復でより多くの更新を行うことができます。具体的には $B$ をバッチサイズとすると、元の勾配降下法では1回しか更新できない所、バッチ処理を行った場合は $N / B$ 回の更新を行うことができます。損失が常に減少しない理由は、評価するたびに異なるデータセットであるためです。データセットからランダムに選定したバッチでは、ある分子が他の分子より予測が難しかったり、1つのバッチに基づいてパラメータを更新しただけなので、各ステップが正しく損失を最小化できているとは限らなかったりします。しかし、バッチがランダムに選定されていると仮定すれば、常に（平均的に）損失の減少量の期待値を向上させることができます。(つまり、損失の期待値を最小化できます)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の標準化\n",
    "\n",
    "損失関数の減少がある一定の値で止まっています。勾配を調べると、非常に大きいものもあれば、非常に小さいものもあることがわかります。ここで重要になってくるのは、それぞれの特徴は大きさが違うことです。例えば、学習で反映されるべきそれぞれの重要さとは無関係に、分子量は比較的大きな数字であり、分子の中の環の数は比較的小さな数字になります。これは学習に影響を与えており、それぞれは同じ学習率、 $\\eta$ を使わなければならないですが、その学習率が適切なものもあれば、小さすぎるものもあるという問題が発生しています。もし、 $\\eta$ を大きくすると、特徴量の勾配からウケる影響が大きくなるため、学習の速度が爆発的に増加してしまいます。そこで、統計の教科書に載っている標準化の式を用いて、全ての素性の大きさを同じにするのが標準的な解決策として取られています。\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{ij} = \\frac{x_{ij} - \\bar{x_j}}{\\sigma_{x_j}}\n",
    "\\end{equation}\n",
    "\n",
    "ここで、$\\bar{x_j}$ は列の平均、$\\sigma_{x_j}$ は列の標準偏差です。訓練データをテストデータで汚染しないように、つまり、訓練するときにテストデータの平均や標準偏差などの情報を使うことがないように、平均と標準偏差の計算には訓練データだけを使うようにします。テストデータは、未知のデータに対するモデルの性能を近似的に示すためのものであり、通常のタスクでは未知のデータがどのような特徴量を持つものであるかは分からないので、標準化のために学習時に使用することはできません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstd = np.std(features, axis=0)\n",
    "fmean = np.mean(features, axis=0)\n",
    "std_features = (features - fmean) / fstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our paramaters\n",
    "# since we're changing the features\n",
    "w = np.random.normal(scale=0.1, size=feature_dim)\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "loss_progress = []\n",
    "eta = 1e-2\n",
    "batch_size = 32\n",
    "N = len(labels)  # number of data points\n",
    "data = (std_features, labels)\n",
    "# compute how much data fits nicely into a batch\n",
    "# and drop extra data\n",
    "new_N = len(labels) // batch_size * batch_size\n",
    "num_epochs = 3\n",
    "\n",
    "# the -1 means that numpy will compute\n",
    "# what that dimension should be\n",
    "batched_features = std_features[:new_N].reshape((-1, batch_size, feature_dim))\n",
    "batched_labels = labels[:new_N].reshape((-1, batch_size))\n",
    "indices = np.arange(new_N // batch_size)\n",
    "\n",
    "# iterate through the dataset 3 times\n",
    "for epoch in range(num_epochs):\n",
    "    # to make it random, we'll iterate over the batches randomly\n",
    "    np.random.shuffle(indices)\n",
    "    for i in indices:\n",
    "        # choose a random set of\n",
    "        # indices to slice our data\n",
    "        grad = loss_grad(w, b, (batched_features[i], batched_labels[i]))\n",
    "        w -= eta * grad[0]\n",
    "        b -= eta * grad[1]\n",
    "        # we still compute loss on whole dataset, but not every step\n",
    "        if i % 50 == 0:\n",
    "            loss_progress.append(loss_wrapper(w, b, data))\n",
    "\n",
    "plt.plot(np.arange(len(loss_progress)) * 50, loss_progress)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が不安定にならないまま、学習率を0.01まで上げることができました。これはすべての特徴が同じオーダーになったことにより可能となっています。また、これらのモデルの改良により、更なる学習を続けることも可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル性能の分析\n",
    "\n",
    "これは重要な事柄なので、後で詳しく調べますが，教師あり学習で通常最初に調べるのは**パリティプロット**で，予測値とラベル予測値を用いた図を作ります．このプロットの良いところは、特徴量の次元に関係なく機能することです。モデルが完全に機能している場合、すべてのデータは $y = \\hat{y}$ 上にプロットされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = linear_model(std_features, w, b)\n",
    "\n",
    "plt.plot([-100, 100], [-100, 100])\n",
    "plt.scatter(labels, predicted_labels, s=4, alpha=0.7)\n",
    "plt.xlabel(\"Measured Solubility $y$\")\n",
    "plt.ylabel(\"Predicted Solubility $\\hat{y}$\")\n",
    "plt.xlim(-13.5, 2)\n",
    "plt.ylim(-13.5, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最終的なモデルの評価は損失の値で行うことができますが、通常、他の指標も使用されます。回帰分析では、損失に加えて**相関係数**が計算されることが多いです。相関係数は次のように計算されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice correlation between predict/labels\n",
    "# from correlation matrix\n",
    "np.corrcoef(labels, predicted_labels)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS USED TO GENERATE A FIGURE\n",
    "# AND NOT RELATED TO CHAPTER\n",
    "# YOU CAN SKIP IT\n",
    "from myst_nb import glue\n",
    "\n",
    "glue(\"corr\", np.round(np.corrcoef(labels, predicted_labels)[0, 1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{glue:}`corr`は相関係数として悪くはないですが、素晴らしいとは言えません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師なし学習\n",
    "\n",
    "教師なし学習では、*ラベルがない状態で* $\\hat{y}$を予測することが目標です。これは不可能なことのように思えますが、どのように成功を判断するのでしょうか。\n",
    "一般に、教師なし学習は3つのカテゴリに分けられます。\n",
    "\n",
    "**クラスタリング**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; このカテゴリでは、 $\\{y_i\\}$ をクラス変数と仮定し、特徴をクラスに分割することを試みます。クラスタリングでは、クラスの定義（クラスタと呼ばれます）と各特徴量がどのクラスタに割り当てられるべきかを同時に学習することになります。\n",
    "\n",
    "```{margin} クラス\n",
    "機械学習において、クラスとは ``dog`` や ``cat`` のようなラベルの一種です。形式的にはラベルの集合（例えば、全ての動物）があり、各特徴ベクトルは1つのクラスまたはクラスの確率分布を持っています。\n",
    "```\n",
    "\n",
    "**シグナルのデノイジング**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; このタスクでは、$x$ はノイズとシグナル（$y$）の2つの成分からできていると仮定し、シグナルの$y$を$x$から抽出し、ノイズを除去することを目標とします。後述する**表現学習**と高い関連性を持ちます。\n",
    "\n",
    "\n",
    "**生成的モデル**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 生成的モデルは、 $P(\\vec{x})$ を学習して、$\\vec{x}$ の新しい値をサンプリングする方法です。これは、$y$を確率とし、それを推定しようとすることに似ています。これらについては、後で詳しく説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリング\n",
    "\n",
    "クラスタリングは歴史的に最もよく知られた機械学習手法の1つであり，今でも良く用いられています。クラスタリングは何もないところにクラスラベルを与えるので、データ中のパターンを見つけ、データから新しい洞察を得るのに役立ちます。そして、化学（そしてほとんどの分野）であまり人気がなくなった理由でもありますが、クラスタリングには正解も不正解もありません。クラスタリングは、二人の人間が独立して行うと、しばしば異なる答えに到達します。とはいえ、クラスタリングは知っておくべきツールであり、良い探索戦略にもなり得ます。\n",
    "\n",
    "```{margin} クラスターラベル\n",
    "クラスタリングには多くの種類があり、中には$y_i$が一体何なのかをぼやかしているものもあります。例えば、クラスタリング手法によっては、$y_i$が割り当てを含まない場合や、$y_i$が1つのクラスではなく、クラスのツリーである場合があります。\n",
    "```\n",
    "\n",
    "ここでは、古典的なクラスタリング手法であるk-meansについて見ていきます。Wikipediaにこの古典的なアルゴリズムに関する[すばらしい記事](https://en.wikipedia.org/wiki/K-means_clustering)があるので、その内容について繰り返すのはやめておきます。クラスタリングの結果を実際に見えるようにするために、特徴量を2次元に投影することから始めます。これは表現学習で詳細に説明されるので、これらのステップについての理解を心配する必要はありません。(訳注: 本当か？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get down to 2 dimensions for easy visuals\n",
    "embedding = sklearn.manifold.Isomap(n_components=2)\n",
    "# only fit to every 25th point to make it fast\n",
    "embedding.fit(std_features[::25, :])\n",
    "reduced_features = embedding.transform(std_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "極端に離れている外れ値もあるので（それはそれで面白いのですが）、データの真ん中99パーセンタイルについて注目していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow, xhi = np.quantile(reduced_features, [0.005, 0.995], axis=0)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.scatter(\n",
    "    reduced_features[:, 0],\n",
    "    reduced_features[:, 1],\n",
    "    s=4,\n",
    "    alpha=0.7,\n",
    "    c=labels,\n",
    "    edgecolors=\"none\",\n",
    ")\n",
    "plt.xlim(xlow[0], xhi[0])\n",
    "plt.ylim(xlow[1], xhi[1])\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Solubility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin} 次元削減\n",
    "特徴ベクトルの$\\vec{x}$を低次元空間に縮退させる操作のことを言います。典型的な例はPCAで、これは線形演算子です。しかし、多くの場合、[t-SNE](https://scikitlearn.org/stable/modules/generated/sklearn.manifold.TSNE.html)のような非線形な方法が好まれます。\n",
    "```\n",
    "\n",
    "次元削減により、特徴量ははわずか2次元となりました。溶解度のクラスによって色を付けることで、いくつかの構造を見ることができます。このような次元削減を行った結果のプロットでは、軸は任意であるため、ラベルを付けないことに注意してください。\n",
    "\n",
    "続けて、クラスタリングを行います。クラスタリングの主な課題は、クラスタをいくつにするか決めることです。いろいろな方法がありますが、基本的には直感に頼ることになります。つまり、化学者として、データ以外の何らかのドメイン知識を使って、クラスタ数を直感的に決める必要があります。非科学的に聞こえますか？だからクラスタリングは難しいんです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# cluster - using whole features\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(std_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とても簡単な手順ですね。では、データを割り当てられたクラスで色付けして可視化してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "point_colors = [f\"C{i}\" for i in kmeans.labels_]\n",
    "plt.scatter(\n",
    "    reduced_features[:, 0],\n",
    "    reduced_features[:, 1],\n",
    "    s=4,\n",
    "    alpha=0.7,\n",
    "    c=point_colors,\n",
    "    edgecolors=\"none\",\n",
    ")\n",
    "# make legend\n",
    "legend_elements = [\n",
    "    plt.matplotlib.patches.Patch(\n",
    "        facecolor=f\"C{i}\", edgecolor=\"none\", label=f\"Class {i}\"\n",
    "    )\n",
    "    for i in range(4)\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.xlim(xlow[0], xhi[0])\n",
    "plt.ylim(xlow[1], xhi[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスター数の選択\n",
    "\n",
    "どうやってクラスターの数を正しく決められたかを判断するのでしょうか？答えは直感です。**エルボープロット**と呼ばれる、学習曲線のように使うことのできるツールがあります。k-meansのクラスタは、クラスタ中心からの平均二乗距離を計算することで損失関数として使うことができます。しかし、クラスタ数を学習可能なパラメータとして扱うと、クラスタ数とデータ点数が等しい(つまり、1つのクラスタに1つのデータが入る)ときに最もフィットすることがわかります。これでは意味がありません。しかし、この損失関数の傾きがほぼ一定になる点が存在し、クラスタを追加することで新しい見識を追加していないと判定することができます。損失をプロットして何が起こるか見てみましょう。時間を節約するために、データセットのうち一部のサンプルを使用していることに気を付けてください。バッチ処理と同じような考え方です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an elbow plot\n",
    "loss = []\n",
    "cn = range(2, 15)\n",
    "for i in cn:\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=i, random_state=0)\n",
    "    # use every 50th point\n",
    "    kmeans.fit(std_features[::50])\n",
    "    # we get score -> opposite of loss\n",
    "    # so take -\n",
    "    loss.append(-kmeans.score(std_features[::50]))\n",
    "\n",
    "plt.plot(cn, loss, \"o-\")\n",
    "plt.xlabel(\"Cluster Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Elbow Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変わり目はどこでしょうか？よくよく見てみると、たぶん、6? 3? 4? 7? 今回は4を選びましょう。響きがいいし、データに基づくともっともらしいからです。\n",
    "最後の作業は、クラスタが実際に何であるかを知ることです。最も中心にあるデータポイント（つまり、クラスタの中心に最も近いデータ）を抽出し、それらをクラスタの代表とみなします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "alt": "Grid of rendered molecular structures that are representative cluster centers"
   },
   "outputs": [],
   "source": [
    "# cluster - using whole features\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(std_features)\n",
    "\n",
    "cluster_center_idx = []\n",
    "for c in kmeans.cluster_centers_:\n",
    "    # find point closest\n",
    "    i = np.argmin(np.sum((std_features - c) ** 2, axis=1))\n",
    "    cluster_center_idx.append(i)\n",
    "cluster_centers = soldata.iloc[cluster_center_idx, :]\n",
    "\n",
    "legend_text = [f\"Class {i}\" for i in range(4)]\n",
    "\n",
    "# now plot them on a grid\n",
    "cluster_mols = [rdkit.Chem.MolFromInchi(inchi) for inchi in cluster_centers.InChI]\n",
    "rdkit.Chem.Draw.MolsToGridImage(\n",
    "    cluster_mols, molsPerRow=2, subImgSize=(400, 400), legends=legend_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、これらのクラスは一体何なのでしょうか？不明です。意図的に溶解度を明らかにしなかった（教師なし学習）ので、必ずしも溶解度と関係があるわけではありません。これらのクラスは、むしろデータセットにどのような特徴が選ばれたかの結果です。クラス1はすべて負電荷であるとか、クラス0は脂肪族であるといった仮説を立てて、検討することができます。しかし、**ベスト**なクラスタリングを選ぶことはできませんし、教師なし学習は洞察やパターンを見つけることが重要で、精度の高いモデルを作ることが重要なのではありません。\n",
    "\n",
    "エルボープロット法は、クラスタ番号を選択するための多くのアプローチの1つです {cite}`pham2005selection`. 私は、直感を利用していることが明確なので、この方法を好んでいます。より洗練された方法は、クラスタリングに正解も不正解もないという事実をある種隠蔽しています。\n",
    "\n",
    "\n",
    "\n",
    "```{note}\n",
    "このプロセスでは、溶解度を予測する関数は得られません。予測されたクラスで溶解度の予測に関する見識を得られるかもしれませんが、それはクラスタリングの目的ではありません。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "* 教師あり機械学習とは、入力の特徴量 $\\vec{x}$ からラベル $y$ を予測するモデルを構築することです。\n",
    "* データはラベル付きでもラベルなしでも適用可能です。\n",
    "* 確率的勾配降下法を用いて、損失を最小化することでモデルを学習できます。\n",
    "* 教師なし学習とは、データのパターンを発見するモデルを作ることです。\n",
    "* クラスタリングは教師なし学習の1つであり、モデルがデータ点をクラスタに分けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習問題\n",
    "\n",
    "### データ処理\n",
    "\n",
    "1. `numpy`の `np.amin`, `np.std` などを使って（`pandas` ではありません！）、すべてのデータ点についての各特徴の平均、最小、最大、標準偏差を計算してください。\n",
    "\n",
    "2. rdkit を使って、分子量の大きい2つの分子を描いてください。また、その構造の変な点を挙げてください。\n",
    "\n",
    "### 線形モデル\n",
    "\n",
    "1. $y = \\vec{w_1} \\cdot \\sin\\left(\\vec{x}\\right) + \\vec{w_2} \\cdot \\vec{x} + b$ のような非線形モデルが線形モデルで表現できることを証明してください。\n",
    "\n",
    "2. 線形モデル方程式をアインシュタインの縮約記法でバッチ式に書き出しなさい。**バッチ形式**とは、バッチを示すインデックスを明示的に持つことである。例えば、ラベルは$y_{bi}$となり、$b$はデータのバッチを、$i$は個々のデータ点を示す。\n",
    "\n",
    "### 損失関数の最小化\n",
    "\n",
    "1. 今回のnotebookでは、特徴量は標準化したが、ラベルは標準化しませんでした。ラベルを標準化することは、学習率の選択に影響を与えるか？説明してください。\n",
    "\n",
    "2. 平均二乗誤差ではなく、平均絶対誤差の損失を実装してください。その勾配を `jax` を使って計算してください。\n",
    "\n",
    "3. 標準化された特徴量を用いて、バッチサイズが学習にどのような影響を与えるかを示してください。バッチサイズは1、8、32、256、1024を使用してください。各実行の間に重みを初期化しなければならないことに注意してください。そして、各バッチサイズでのlog-lossを同じプロット上にプロットし、その結果を説明してください。\n",
    "\n",
    "### クラスタリング\n",
    "\n",
    "1. クラスタリングは教師なし学習の一種であり、ラベルを予測すると述べました。クラスタリングで予測されるラベルとは、具体的にどのようなものでしょうか。いくつかのデータポイントについて、予測されるラベルがどのようなものか書き出してみてください。\n",
    "\n",
    "2. クラスタリングでは、特徴量からラベルを予測します。ラベルがあっても、それを特徴量と見なしてクラスタリングすることができます。このように、ラベルを特徴量として扱い、クラスを表す新しいラベルを予測しようとするクラスタリングが良くない理由を2つ述べてください。\n",
    "\n",
    "3. Isomapプロット（縮小次元プロット）上で、点がどのグループに属するか（G1、G2など）で色分けしてください。これとクラスタリングの間に何か関係があるでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cited References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c954abbd500635c929c825ee4f107e63a7c5cb2bc239f8f781b23be55e7a62ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
