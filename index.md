---
html_meta:
  "description lang=en": "Deep Learning for Molecules & Materials Book"
  "property=og:locale": "en_US"
  "twitter:card": "summary"
  "twitter:description": "Deep Learning for Molecules & Materials Book"
  "twitter:title": "dmol.pub ğŸ“–"
  "twitter:image": "https://dmol.pub/_static/robot-chem.png"
  "twitter:site": "@andrewwhite01"
---
![Picture of art installation of networked cables](_static/images/header.png)

# æ¦‚è¦

åŒ–å­¦ã‚„ææ–™ç§‘å­¦ã®åˆ†é‡ã«ãŠã„ã¦ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ¨™æº–çš„ãªãƒ„ãƒ¼ãƒ«ã«ãªã‚Šã¤ã¤ã‚ã‚Šã¾ã™ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ã€ã‚ã‚‹å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡ï¼‰ã¨å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¨˜è¿°ã•ã‚Œã‚‹é–¢æ•°ã§çµã³ã¤ã‘ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ï¼ˆæ•°å€¤çš„ã«ï¼‰å¾®åˆ†å¯èƒ½ã§ã‚ã‚Šã€ã‚ã‚‰ã‚†ã‚‹é–¢æ•°ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ãŒã§ãã‚‹å¼·åŠ›ãªé“å…·ã§ã™ã€‚ä¾‹ãˆã°ã€åˆ†å­ã®æ§‹é€ ã¨æ©Ÿèƒ½ã‚’çµã³ã¤ã‘ã‚‹ã“ã¨ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¤å…¸çš„ãªæ´»ç”¨ã®ä¸€ã¤ã§ã™ã€‚[æœ€è¿‘ã®ä¾‹](https://doi.org/10.1039/C6SC05720A)ã§ã¯ã€é‡å­è¨ˆç®—ã‚’åŠ‡çš„ã«é«˜é€ŸåŒ–ã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§DFTè¨ˆç®—ãƒ¬ãƒ™ãƒ«ã®é«˜ã„ç²¾åº¦ã‚’å®Ÿç¾ã§ãã‚‹ã»ã©ã«ãªã£ã¦ã„ã¾ã™ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç‰¹ã«æ³¨ç›®ã•ã‚Œã‚‹ã®ã¯ã€ã“ã‚Œã¾ã§é›£è§£ã ã£ãŸé–¢æ•°ã‚’è¿‘ä¼¼ã™ã‚‹ãŸã‚ã®å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹ã“ã¨ã€ **ãã‚Œã«åŠ ãˆã¦ã€** æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’ã‚‚ã¤ãŸã‚ã§ã™ã€‚

ã“ã®æœ¬ã§ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã€ã“ã‚Œã¾ã§å¤å…¸çš„ãªæ©Ÿæ¢°å­¦ç¿’ã§ã¯å®Ÿç¾ä¸å¯èƒ½ã ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ§‹æˆã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã®é›†åˆä½“ã¨ã—ã¦æ‰ãˆã¦ã„ãã¾ã™ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¤å…¸çš„ãªæ©Ÿæ¢°å­¦ç¿’ã®å¤§ããªé•ã„ã¯ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¿…è¦æ€§ã§ã™ã€‚å¤å…¸çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€å¤šãã®å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ã©ã®ã‚ˆã†ãªç‰¹å¾´ãŒé‡è¦ã§ã€ãã®ç‰¹å¾´ã‚’åˆ†å­ã‹ã‚‰ã©ã®ã‚ˆã†ã«è¨ˆç®—ã™ã‚‹ã‹ã®è¨­è¨ˆï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰ãŒå¿…è¦ã§ã—ãŸã€‚ç”¨ã„ã‚‹ç‰¹å¾´é‡ã¯äºˆæ¸¬æ€§èƒ½ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‹ã‚‰ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¯ç ”ç©¶è€…ã®é ­ã‚’æ‚©ã¾ã›ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸€ã¤ã§ã—ãŸã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã€é€šå¸¸ã€end-to-endã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚ã™ãªã‚ã¡ã€ãƒ‡ãƒ¼ã‚¿ã®ã©ã®ç‰¹å¾´ãŒé‡è¦ã§ã‚ã‚‹ã‹ã¨ã„ã†åˆ¤æ–­ã¯ã‚‚ã¯ã‚„å¿…è¦ç„¡ãã€åˆ†å­æ§‹é€ ã‚’ç›´æ¥æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã®ã§ã™ã€‚

ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã“ã‚Œã»ã©æ™®åŠã™ã‚‹ã“ã¨ã«ãªã£ãŸã‚‚ã†ä¸€ã¤ã®ç†ç”±ã¯ã€ãã®æŸ”è»Ÿæ€§ã¨ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®æˆç†Ÿæ€§ã«ã‚ã‚Šã¾ã™ã€‚ä»¥å‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«æ–°ã—ã„æ–¹ç¨‹å¼ã‚’å°å‡ºãƒ»å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã£ãŸãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ä½¿ç”¨ã¯é¢å€’ãªã‚‚ã®ã§ã—ãŸã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã¨ã„ã†ä¸€ã¤ã®ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢ã ã‘ã§æ§˜ã€…ãªå•é¡Œã«å¯¾å¿œã§ãã‚‹ã“ã¨ã‹ã‚‰ã€å•é¡Œã”ã¨ã«æ–°ãŸãªæ‰‹æ³•ã‚’è€ƒãˆã‚‹å¿…è¦ãŒãªããªã‚Šã€ã¾ãŸãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´ã‚‚é¥ã‹ã«ç°¡å˜ã«ãªã‚Šã¾ã—ãŸã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ç§‘å­¦ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚‚ãªã‘ã‚Œã°ã€ç§‘å­¦è€…ã®ä»£ã‚ã‚Šã§ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚åˆ†å­ã‚„ææ–™ã«é©ç”¨ã™ã‚‹æº–å‚™ãŒã§ããŸæˆç†Ÿã—ãŸãƒ„ãƒ¼ãƒ«ãªã®ã§ã™ã€‚

## æƒ³å®šã™ã‚‹èª­è€…

æœ¬æ›¸ã®å¯¾è±¡èª­è€…ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨åŒ–å­¦ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’æŒã¡ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä¸­å¿ƒã¨ã—ãŸMaterials InformaticsæŠ€è¡“ã®ç¿’å¾—ã«èˆˆå‘³ã®ã‚ã‚‹å­¦ç”Ÿã§ã™ã€‚ä¾‹ãˆã°ã€åŒ–å­¦ã‚„ææ–™ç§‘å­¦ã®å¤§å­¦é™¢ç”Ÿã‚„ä¸Šç´šå­¦éƒ¨ç”Ÿã§ã€ã‚ã‚‹ç¨‹åº¦ã®Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®ã‚¹ã‚­ãƒ«ã‚’æŒã¤äººã¯ã€æœ¬æ›¸ã®æ©æµã‚’å—ã‘ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³Aã¨Bã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åŸç†ã‚’æ•™è‚²çš„ã«ç´¹ä»‹ã—ã¦ã„ã¾ã™ãŒã€æ©Ÿæ¢°å­¦ç¿’ã®å…¨ã¦ã‚’ç¶²ç¾…ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã‚’å–ã‚Šä¸Šã’ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€æ±ºå®šæœ¨ã‚„SVMãªã©ã®ãƒˆãƒ”ãƒƒã‚¯ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç†è§£ã™ã‚‹ä¸Šã§é‡è¦ã§ã¯ãªã„ãŸã‚ã€ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³Cã§ã¯ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŸç†ã¨ã€é‡è¦ãª{doc}`dl/gnn`ã‚„{doc}`dl/VAE`ãªã©ã®å…·ä½“çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è©³ç´°ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ãã®ä»–ã®ç« ã§ã¯ã€{doc}`dl/NLP`ã®ã‚ˆã†ã«ã€åŒ–å­¦ã‚„ææ–™ç§‘å­¦ã‚’å¯¾è±¡ã¨ã—ãŸã€ã‚ˆã‚Šå¤§ããªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦æ¦‚è¦ã¨ã‚µãƒ¼ãƒ™ã‚¤ã‚’æä¾›ã—ã¾ã™ã€‚æœ€å¾Œã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³Dã§ã¯ã€åŒ–å­¦ã¨ææ–™ç§‘å­¦ã«ãŠã‘ã‚‹å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¿œç”¨ä¾‹ã¨ã—ã¦ã€ã‚ˆã‚Šç™ºå±•çš„ãªä¾‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†’é ­ã§å¿…è¦ãªèƒŒæ™¯çŸ¥è­˜ã‚’è¿°ã¹ã¦ã„ã¾ã™ãŒã€æœ¬æ›¸å…¨ä½“ã‚’é€šã—ã¦Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬çš„ãªã‚¹ã‚­ãƒ«ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚åŒ–å­¦ã«ç‰¹åŒ–ã—ãŸPythonã®å…¥é–€æ›¸ã¯ã€Molecular Sciences Software Instituteã®[ãƒªã‚½ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸](http://education.molssi.org/resources.html#programming)ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
è¨³æ³¨ï¼šæ—¥æœ¬èªã§åˆ©ç”¨ã§ãã‚‹å„ªã‚ŒãŸPythonã®å…¥é–€ãƒªã‚½ãƒ¼ã‚¹ã®ä¸€ã¤ã«ã¯ã€æ±äº¬å¤§å­¦ æ•°ç†ãƒ»æƒ…å ±æ•™è‚²ç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼ãŒå…¬é–‹ã—ã¦ã„ã‚‹[Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å…¥é–€ \#utpython](https://utokyo-ipp.github.io/index.html) ã®è¬›ç¾©è³‡æ–™ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚

## Framework Choice

Deep learning is always a little tied up in the implementation details -- it's hard to grasp without seeing code. Thus, framework choice can be a part of the learning process. This book assumes familiarity with Python and `numpy` and we use exclusively Python. For the deep learning framework, we use `Jax`, `Tensorflow`, `Keras`, and `scikit-learn` for different purposes. `Jax` is easy to learn because it's essentially `numpy` with automatic differentiation and GPU/TPU-acceleration. In this book, we use `Jax` when it's important to understand the implementation details and connect the equations to the code. `Keras` is a high-level framework that has many common deep learning features implemented. It is used when we would like to work with more complex models and I'm trying to show a more complete model. Of course, you can use `Jax` for complete models and show detailed implementations in `Keras`. This is just my reasoning for the choice of framework. `scikit-learn` is an ML package and thus we'll see in the early chapters on ML. Finally, `Tensorflow` is the underlying library of `Keras` so if we want to implement new layers in `Keras` we do it through `Tensorflow`. `TensorflowProbability` is an extension to `Tensorflow` that supports random variables and probability distributions used in our generative models. The most important framework left out of this book is `PyTorch`, which has recently taken the lead to be the most popular framework in deep learning research (not necessarily industry). Ultimately, this book presents the equations and implementation details so that you will learn concepts that are independent of the framework. You should thus be able to quickly pick up `PyTorch`, MXNet, or whatever the next new framework might be.

One of the most common mistakes I see from students is that they try to learn deep learning via web searching questions and reading documentation. *This is a terrible way to learn deep learning.* There is quite a bit of information out there, but you will end up with a distorted and framework-specific understanding of deep learning. Remember, a high-ranking search result may be relevant and popular, but that doesn't mean it will help you learn. More importantly, learning deep learning through blogs and Stack overflow makes it so hard to grasp the mathematics and intuition. Web searching and hacking together code is definitely a part of deep learning (for better or worse), but you should do this once you have a firm grasp of the math and details of the model you want to implement.

## Interactivity

On each chapter, you'll see the &nbsp;<i aria-label="Launch interactive content" class="fas fa-rocket"></i>&nbsp; button on the top. This launches the chapter as an interactive Google Colab. Each chapter also includes notes on the packages that may need to be installed. If you have problems with install, the complete current list of packages for the textbook is available [here](https://github.com/whitead/dmol-book/blob/master/package/requirements.txt).

When using interactivity, many of the chapter will benefit from using a graphics processing unit (GPU). GPUs are what makes deep learning fast enough to be practical on large dataset. This is possible in Google Colab, but may require additional steps if running this locally. Check the documentation of the package you're using (e.g., `Jax`, `PyTorch`, `Tensorflow`) to find out how to use a GPU locally. I have carefully constructed the examples to be small enough though to run on a normal CPU in a laptop though, so the GPU is optional.


## Example models

Here are the major models we will construct learn to implement in this book: *Sorry, but I'm unable to link directly to the examples, so you'll need to scroll to them.*

1. We explore predicting solubility of molecules with {doc}`graph convolutional neural networks<dl/gnn>`, {doc}`recurrent neural networks<dl/NLP>`, {doc}`dense neural networks<dl/xai>`, and {doc}`kernel learning<ml/kernel>`.
2. We implement a SchNet model to {doc}`predict what space group a structure belongs to<dl/gnn>`.
3. We implement a Recurrent Neural Network to {doc}`predict the solubility of proteins/peptides<dl/layers>` and {doc}`predict the if a peptide will lyse red blood cells<dl/xai>`.
4. We predict the DFT single-point energy of molecules {doc}`with a graph convolutional neural network<applied/QM9>`.
5. We propose new molecules with a generative {doc}`recurrent neural network<applied/MolGenerator>`.
6. We learn to align and embed polymer trajectories with {doc}`variational autoencoders<dl/VAE>` and {doc}`equivariant data representations<dl/data>`.
7. We classify if molecules are likely to be toxic with {doc}`logistic regression<ml/classification>`.

and there are many smaller examples throughout the book.

## Table of Contents

```{tableofcontents}
```

## Contributors

Thank you to contributors for offering suggestions, identifying errors, and helping improve this book! Twitter handles, if available

### Contributed Chapter

1. Mehrad Ansari (@MehradAnsari)

### Contributed Content to Chapter

1. Geemi Wellawatte (@GWellawatte)

### Substantial Feedback on Content

1. Lily Wang (@lilyminium)
2. Marc Finzi (@m_finzi)
3. Kevin Jablonka (@kmjablonka)
4. Elana Simon
5. Cathrine Bergh (@cathrinebergh)

### Code Fixes, Math Fixes, Language Fixes

1. Oion Akif
2. Heta Gandhi (@gandhi_heta)
3. Mattias Hartveit
4. Andreas KrÃ¤mer
5. Mehrad Ansari (@MehradAnsari)
6. Ritsuya Niwayama
7. Varsha Jain
8. Simon Duerr
9. Julia Westermayr (@JWestermayr)
10. Ernest Awoonor-Williams
11. Joshua Schrier (@joshuaschrier)
12. Marin Bukov
13. Arun Pa Thiagarajan (@arunppsg)
14. Ankur Parmar
15. Erik Thiede (@erik_der_elch)

## Citation

Please cite the [livecommsj overview article](https://doi.org/10.33011/livecoms.3.1.1499):

```bibtex
@article{white2021deep,
  title={Deep Learning for Molecules and Materials},
  journal={Living Journal of Computational Molecular Science},
  author={White, Andrew D},
  url={https://dmol.pub},
  year={2021},
  volume={3},
  number={1},
  pages={1499},
  doi={10.33011/livecoms.3.1.1499}
}
```

## Funding Support

Research reported in this work was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number R35GM137966. This material is based upon work supported by the National Science Foundation under Grant No. 1764415.

## License (CC BY-NC 3.0)

Creative Commons Legal Code

Attribution-NonCommercial 3.0 Unported

See complete description of license at [https://creativecommons.org/licenses/by-nc/3.0/](https://creativecommons.org/licenses/by-nc/3.0/) or at repo [https://github.com/whitead/dmol-book](https://github.com/whitead/dmol-book)
